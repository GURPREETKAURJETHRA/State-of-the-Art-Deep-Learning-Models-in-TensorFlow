{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceXcxnFyONlJ"
      },
      "source": [
        "# Build TensorFlow Input Pipelines\n",
        "\n",
        "The **tf.data** API enables creation of complex input pipelines from simple, reusable pieces. It facilitates handling of large amounts of data, reading from different data formats, and performing complex transformations. The tf.data API introduces a **tf.data.Dataset** abstraction that represents a sequence of elements where each element consists of one or more components.\n",
        "\n",
        "Comprehensive resource on the tf.data.Dataset:\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "\n",
        "A TensorFlow dataset can be created in two distinct ways. A data source constructs a dataset from data stored in memory or in one or more files. Or, a data transformation constructs a dataset from one or more tf.data.Dataset objects.\n",
        "\n",
        "Comprehensive resource on TensorFlow input pipelines:\n",
        "\n",
        "https://www.tensorflow.org/guide/data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3DlP03sy1fv"
      },
      "source": [
        "# Basic Mechanics\n",
        "\n",
        "To create an input pipeline, start with a data source. We can construct a dataset from data in memory with from_tensors() or from_tensor_slices().Alternatively, we can use TFRecordDataset() for data stored in a file in the recommended TFRecord format.\n",
        "\n",
        "Once a data source is created, we transform it into a new dataset by chaining method calls on the tf.data.Dataset object. The dataset object is a Python iterable that can be consumed with a for loop.\n",
        "\n",
        "Comprehensive resource on performance optimization with the tf.data API:\n",
        "\n",
        "https://www.tensorflow.org/guide/data_performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FgOD1CCcid9"
      },
      "source": [
        "# Google Developers Codelabs\n",
        "\n",
        "Even after you work through the examples in this book, you may want to add to your deep learning application knowledge by exploring additional tutorials. **Google Developers Codelabs** provide guided tutorials emphasizing hands-on coding examples. Most codelabs step you through the process of building a small application or adding a new feature to an existing application. They cover a wide range of topics such as Android Wear, Google Compute Engine, Project Tango, and Google APIs on iOS.\n",
        "\n",
        "To peruse the Codelabs website, visit:\n",
        "\n",
        "https://codelabs.developers.google.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icfKUJXt1RTF"
      },
      "source": [
        "# Import **tensorflow** Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAOMMET8UIFT"
      },
      "source": [
        "Import tensorflow library and alias as **tf**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61CYgc49HwN5"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnIrKGSX0Zkx"
      },
      "source": [
        "# GPU Hardware Accelerator\n",
        "\n",
        "To vastly speed up processing, we can use the GPU available from the Google Colab cloud service. Colab provides a free Tesla K80 GPU of about 12 GB. Itâ€™s very easy to enable the GPU in a Colab notebook:\n",
        "\n",
        "1.\tclick **Runtime** in the top left menu\n",
        "2.\tclick **Change runtime** type from the drop-down menu\n",
        "3.\tchoose **GPU** from the Hardware accelerator drop-down menu\n",
        "4.\tclick **SAVE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2S6yy-suUut"
      },
      "source": [
        "Verify that GPU is active:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRLvQJxzuiph"
      },
      "source": [
        "tf.__version__, tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ElVm1Ji06To"
      },
      "source": [
        "If '/device:GPU:0' is displayed, the GPU is active. If '..' is displayed, the regular CPU is active."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3adMsVH2BBg"
      },
      "source": [
        "# Create a TensorFlow Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7c8jG90URwQ"
      },
      "source": [
        "Create a dataset with three tensors with six elements each:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcR3m1nC0KAq"
      },
      "source": [
        "data = [[8, 5, 7, 3, 9, 1],\n",
        "        [0, 3, 1, 8, 5, 7],\n",
        "        [9, 9, 9, 0, 0, 7]]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc1BcUEs2lvi"
      },
      "source": [
        "We transformed a dataset into a tf.data.Dataset object with **from_tensor_slices**, which creates a dataset with a separate element for each row of the input tensor. The shape is (6,), which means that each row contains 6 scalar values. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxXETNJP18og"
      },
      "source": [
        "# Consume the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8R0MYFVUcmC"
      },
      "source": [
        "Display tensor information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi_ILWlZ12zm"
      },
      "source": [
        "for i, row in enumerate(dataset):\n",
        "  print ('row ' + str(i), ':', end=' ')\n",
        "  print (row.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BG_QgRD2vuY"
      },
      "source": [
        "The tf.data.Dataset object is a Python iterable that we consume with a for loop. The **numpy()** method explicitly converts a Tensor to a numpy array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXTecTm1Upsb"
      },
      "source": [
        "Alternatively, we can use the **take** method with a tf.data.Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE1lB5qMUkPr"
      },
      "source": [
        "for e in dataset.take(3):\n",
        "  print (e.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3WCpkpo3BaI"
      },
      "source": [
        "Another option is to create a Python iterator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETR1Kqu43BhB"
      },
      "source": [
        "# create iterator\n",
        "it = iter(dataset)\n",
        "\n",
        "# display elements\n",
        "print (next(it).numpy())\n",
        "print (next(it).numpy())\n",
        "print (next(it).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z40koTdm3JeR"
      },
      "source": [
        " Use **iter** to create a Python iterator. Consume iterator elements with the **next** method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-2oD7FH5rAU"
      },
      "source": [
        "# Dataset Structure\n",
        "\n",
        "The **Dataset.element_spec** allows inspection of the dataset. The property returns a nested structure of tf.TypeSpec objects that match the structure of the element, which may be a single component, a tuple of components, or a nested tuple of components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBMnJd4s7tTn"
      },
      "source": [
        "Inspect the dataset and its type:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMlzpABy5UpM"
      },
      "source": [
        "dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPrr6m15VQbw"
      },
      "source": [
        "Alternatively, we can just display the tf.data.Dataset object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmFVEKXBVQpt"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaqKRAGN8X5R"
      },
      "source": [
        "# Create a Dataset from Memory\n",
        "\n",
        "If all of your input data fits in memory, the simplest way to create a Dataset is to convert it to tf.Tensor objects and use Dataset.from_tensor_slices()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyjBuNvOIOXX"
      },
      "source": [
        "## Load Data into Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECN0sNbEVfLN"
      },
      "source": [
        "Load train and test data directly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6QdKGrj8YBA"
      },
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1yvJuN-xK4F"
      },
      "source": [
        "Inspect types:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5FT3EN-xK-E"
      },
      "source": [
        "type(train[0]), type(train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "savAKtJzxXLz"
      },
      "source": [
        "Train and test sets are tuples. For both, the first tuple element contains feature images and the second contains labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBZXpsFW_hE6"
      },
      "source": [
        "## Access Images and Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfBseqE1VjUd"
      },
      "source": [
        "Load images and labels into variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHY-vDdD_lJC"
      },
      "source": [
        "train_img, train_lbl = train\n",
        "test_img, test_lbl = test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjE_RH2AATLE"
      },
      "source": [
        "## Inspect Shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quCc9mDoVpR1"
      },
      "source": [
        "Display train and test shapes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPxb3jEZATRC"
      },
      "source": [
        "print ('train:', train_img.shape, train_lbl.shape)\n",
        "print ('test:', test_img.shape, test_lbl.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-sgIreUVs-m"
      },
      "source": [
        "Train data consists of 60,000 28 x 28 feature images and 60,000 labels. Test data consists of 10,000 28 x 28 feature images and 10,000 labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEDo8B1DAqgh"
      },
      "source": [
        "## Scale and Create tf.data.Dataset\n",
        "\n",
        "Scale data for efficient processing and create Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vyQfpQJ_hKR"
      },
      "source": [
        "train_image = train_img / 255.0\n",
        "test_image = test_img / 255.0\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_image, train_lbl))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_image, test_lbl))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CRdpqwWV59G"
      },
      "source": [
        "Feature image pixel values are typically integers that range from 0 to 255. To scale, divide feature images by 255 to get pixels values that range from 0 to 1.\n",
        "\n",
        "Scaling images is a critical preprocessing step because deep learning models train faster on smaller images. Moreover, many deep learning model architectures require that our images are the same size. But raw images may vary in size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG1GHzunsErL"
      },
      "source": [
        "## Verify Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiaeevz_YQQ2"
      },
      "source": [
        "Let's see if scaling worked as expected. Display a tensor a prescaled tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTt-P2qxsEwk"
      },
      "source": [
        "train_img[0][3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuJDdQD6arEZ"
      },
      "source": [
        "Display the same tensor after it is scaled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xZLzD6ysj82"
      },
      "source": [
        "train_image[0][3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw27QVopYunh"
      },
      "source": [
        "VoilÃ !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbfnrKsazAp"
      },
      "source": [
        "## Check Image Shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtJn7V--W-YH"
      },
      "source": [
        "Fashion-MNIST images are sized equally. But, let's check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlzxseZnW-gt"
      },
      "source": [
        "for img, lbl in train_ds.take(5):\n",
        "  print ('image shape:', img.shape, end=' ')\n",
        "  print ('label:', lbl.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-AnAnCxYGn0"
      },
      "source": [
        "We don't need to resize images because they sized equally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2PDI7_6BX_A"
      },
      "source": [
        "## Inspect Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3qYLobXYYLd"
      },
      "source": [
        "Check tf.data.Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZFX7cUOBYGh"
      },
      "source": [
        "train_ds, test_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEzmpQJUOzfq"
      },
      "source": [
        "## Create Variable to Hold Input Shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5-DSlsfZHLY"
      },
      "source": [
        "Assign variable feature image shape for use in model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LV9PvKhOzne"
      },
      "source": [
        "for img, _ in train_ds.take(1):\n",
        "  img.shape\n",
        "\n",
        "img_shape = img.shape\n",
        "img_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoAn5BCVwx6G"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjfMC6DlCir7"
      },
      "source": [
        "Visualize an element from the train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-q2gE0-CiyG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for feature, label in train_ds.take(1):\n",
        "  plt.imshow(feature, cmap='ocean')\n",
        "plt.axis('off')\n",
        "plt.grid(b=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Dn-zSgydqq"
      },
      "source": [
        "## Define Class Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inoYVLVzbQij"
      },
      "source": [
        "From experience working with Fashion-MNIST, we know the corresponding label names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djWJKrRaydwn"
      },
      "source": [
        "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
        "                'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag',\n",
        "                'Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tumkk9SfxqB"
      },
      "source": [
        "## Convert Numerical Label to Class Label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au_GEjmfbdTz"
      },
      "source": [
        "Labels are numerical in the tf.data.Dataset, but we can see the corresponding class name using the **class_labels** list we just created:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCoIYflQfj9Q"
      },
      "source": [
        "for _, label in train_ds.take(1):\n",
        "  print ('numerical label:', label.numpy())\n",
        "print ('class label:', class_labels[label.numpy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyhIpO_1cHkQ"
      },
      "source": [
        "##Create a Plot of Examples from the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5DFoiYJwumG"
      },
      "source": [
        "Select images from the train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7QOWjdFwus9"
      },
      "source": [
        "num = 30\n",
        "images, labels = [], []\n",
        "for feature, label in train_ds.take(num):\n",
        "  images.append(tf.squeeze(feature.numpy()))\n",
        "  labels.append(label.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoQNd8RjwuyL"
      },
      "source": [
        "Create a function to display a grid of examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhA45g4Iwu3e"
      },
      "source": [
        "def display_grid(feature, target, n_rows, n_cols, cl):\n",
        "  plt.figure(figsize=(n_cols * 1.5, n_rows * 1.5))\n",
        "  for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "      index = n_cols * row + col\n",
        "      plt.subplot(n_rows, n_cols, index + 1)\n",
        "      plt.imshow(feature[index], cmap='twilight',\n",
        "                 interpolation='nearest')\n",
        "      plt.axis('off')\n",
        "      plt.title(cl[target[index]], fontsize=12)\n",
        "  plt.subplots_adjust(wspace=0.2, hspace=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o0F8Ul6wu9O"
      },
      "source": [
        "Invoke the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcfT4XwlwvCP"
      },
      "source": [
        "rows, cols = 5, 6\n",
        "display_grid(images, labels, rows, cols, class_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFx_kbT1zUbf"
      },
      "source": [
        "# Build the Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZUr6CFdjmoU"
      },
      "source": [
        "## Configure Dataset for Performance\n",
        "\n",
        "Use buffered prefetching and caching to improve I/O performance. Shuffle data to ensure a better overall model.\n",
        "\n",
        "**Prefetching** overlaps the preprocessing and model execution of a training step. While the model is executing training step **s**, the input pipeline is reading the data for step **s+1**. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data. The tf.Dataset.prefetch transformation overlaps data preprocessing and model execution while training.\n",
        "\n",
        "The **tf.data.Dataset.cache** keeps the images in memory after they're loaded off disk during the first epoch, which ensures that the dataset does not become a bottleneck while training your model. As a result, caching save some operations (like file opening and data reading) from being executed during each epoch.\n",
        "\n",
        "**Shuffling** data serves the purpose of reducing variance and making sure that models remain general and overfits less. The obvious case where you'd shuffle your data is if your data is sorted by their class/target. Here, you will want to shuffle to make sure that your training/test/validation sets are representative of the overall distribution of the data.\n",
        "\n",
        "Resource:\n",
        "\n",
        "https://www.tensorflow.org/guide/data_performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpTj-2BHzUi0"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "SHUFFLE_SIZE = 5000\n",
        "\n",
        "train_f = train_ds.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)\n",
        "train_fm = train_f.cache().prefetch(1)\n",
        "\n",
        "test_f = test_ds.batch(BATCH_SIZE)\n",
        "test_fm = test_f.cache().prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sd-0RGE0SWM"
      },
      "source": [
        "Shuffle train data. Batch, cache, and prefetch train and test data. Adding **cache()** increases performance because data is read and written only once during the first epoch rather than during each epoch. Adding **prefetch(1)** is a good idea because it adds efficiency to the batching process. That is, while our training algorithm is working on one batch, TensorFlow is working on the dataset in parallel to get the next batch ready. So, prefectch can dramatically improve training performance.\n",
        "\n",
        "We set batch size and shuffle size based on trial and error experiments. You can experiment by adjusting batch and shuffle sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV1s0ZRGzUp8"
      },
      "source": [
        "Inspect tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3lkoX4_zUyc"
      },
      "source": [
        "train_fm, test_fm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htGcv-Eg0rpa"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6xDNz04044T"
      },
      "source": [
        "Import requisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhtXXF8307Zz"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owi82mdbdaKM"
      },
      "source": [
        "Clear previous models and generate a seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jxyP2mFdaPX"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8drHGDn1Baz"
      },
      "source": [
        "Create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOD_a5nQ0rvz"
      },
      "source": [
        "model = Sequential([\n",
        "  Flatten(input_shape=img_shape),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dropout(0.4),\n",
        "  Dense(10, activation=None)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krTlYWtk1Zvt"
      },
      "source": [
        "The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n",
        "\n",
        "Most of deep learning consists of chaining together simple layers. Most layers, such as *Dense*, have parameters that are learned during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBhrLkvQ1ha8"
      },
      "source": [
        "The first layer in this network is a **Flatten** layer, which transforms the format of the images from a two-dimensional array (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn because it only reformats the data.\n",
        "\n",
        "After the pixels are flattened, the network consists of a sequence of two Dense layers. **Dense** layers are fully connected neural layers, which means that all the neurons in a layer are connected to those in the next layer.\n",
        "\n",
        "The first Dense layer has 128 nodes (or neurons). We add a **Dropout** layer after the first Dense layer to reduce overfitting. The second (and last) layer returns a logits array with length of 10. Each node contains a score that indicates that the current image belongs to one of the 10 classes.\n",
        "\n",
        "**Dropout** is a regularization method that approximates training a large number of neural networks with different architectures in parallel. During training, some number of layer outputs are randomly ignored or 'dropped out', which has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different 'view' of the configured layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTXAAw-x1Mxr"
      },
      "source": [
        "Inspect model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN43YvT_1M4m"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6lNVy8s1SmS"
      },
      "source": [
        "# Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6lLgNiSf6op"
      },
      "source": [
        "Compile the model with **SparseCategoricalCrossentropy** loss. Sparse categorical cross entropy performs well when classes are mutually exclusive. That is, each sample belongs exactly to one class. An advantage of using sparse categorical cross entropy is that it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n",
        "\n",
        "The from_**logits=True** attribute informs the loss function that the output values generated by the model are not normalized. That is, the softmax function has not been applied on them to produce a probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b67AWdMB1SsM"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC257td92Cp2"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZNfLAfBiqQn"
      },
      "source": [
        "Train the model with 10 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hst3gnsZ2CwF"
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit(train_fm, epochs=epochs,\n",
        "                    verbose=1, validation_data=test_fm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heg52ckETj6E"
      },
      "source": [
        "# Load a TensorFlow Dataset (TFDS) in a Single Batch\n",
        "\n",
        "This is an **excellent** way to conveniently convert TFDS to numpy arrays. TFDS is covered in detail in a later chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyfNx24BPk7K"
      },
      "source": [
        "## Create Numpy Datasets with tfds.load\n",
        "\n",
        "We load the **MNIST** dataset for this example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C7uTHjWTM2w"
      },
      "source": [
        "Create numpy train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_Nn5nYPpISM"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "image_train, label_train = tfds.as_numpy(\n",
        "    tfds.load(\n",
        "        'mnist', split='train',\n",
        "        batch_size=-1, as_supervised=True,\n",
        "        try_gcs=True))\n",
        "\n",
        "type(image_train), image_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz9HhkrYqBmb"
      },
      "source": [
        "By using *batch_size=-1*, we load the full dataset in a single batch. *tfds.load* returns a dictionary by default, a tuple with *as_supervised=True* of *tf.Tensor* or a *np.array* with *tfds.as_numpy*.\n",
        "\n",
        "The train set contains 60,000 28 x 28 images. The **1** dimension indicates that the data is grayscale. A **grayscale** image is one in which the only colors are shades of gray. That is, it only contains luminance (or brightness) information and no color information.\n",
        "\n",
        "Be careful that your dataset can fit in memory and that all examples have the same shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XVO75QpqH3B"
      },
      "source": [
        "Create numpy test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr1DTvCkqH8L"
      },
      "source": [
        "image_test, label_test = tfds.as_numpy(\n",
        "    tfds.load(\n",
        "        'mnist', split='test',\n",
        "        batch_size=-1, as_supervised=True,\n",
        "        try_gcs=True))\n",
        "\n",
        "type(image_test), image_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb2E-zjqPx5A"
      },
      "source": [
        "## Inspect Shapes and Pixel Intensity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DZb66yKqqYv"
      },
      "source": [
        "Get shapes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1COU8GQqcu6U"
      },
      "source": [
        "image_train.shape, label_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5qHN7ObeEH3"
      },
      "source": [
        "image_test.shape, label_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAIUFxCFxjwr"
      },
      "source": [
        "Create a function to find the first pixel vector with pixel intensity values. We do this because many of the vectors have zero values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx9BN-CAb-Ly"
      },
      "source": [
        "def find_intensity(m):\n",
        "  for i, vector in enumerate(m):\n",
        "    for j, pixels in enumerate(vector):\n",
        "      if pixels > 0:\n",
        "        print (vector)\n",
        "        return i, j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bg91Usmzc_5"
      },
      "source": [
        "Invoke the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PabRpcEzdFF"
      },
      "source": [
        "M = image_train[0]\n",
        "indx = find_intensity(M)       \n",
        "image_train[0][indx[0]][indx[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ihtJh7Wxu7A"
      },
      "source": [
        "The non-zero values are pixel intensities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8aCNgHNxzxi"
      },
      "source": [
        "Display the first pixel with intensity greater than zero:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w-kFlTHdNmb"
      },
      "source": [
        "image_train[0][indx[0]][indx[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcSnRmlRs7Ad"
      },
      "source": [
        "## Scale\n",
        "\n",
        "Since numpy array values are float, we can divide by 255. to scale the pixels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5rJRI0KtlQW"
      },
      "source": [
        "train_sc = image_train / 255.0\n",
        "test_sc = image_test / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG0MWwqS8LKI"
      },
      "source": [
        "Verify scaling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF7Vo-rQt1eo"
      },
      "source": [
        "image_train[0][indx[0]][indx[1]], train_sc[0][indx[0]][indx[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8vQ_ZPiP6GU"
      },
      "source": [
        "## Prepare Data for TensorFlow Consumption"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4-GYAPIuBT0"
      },
      "source": [
        "Slice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nItXu54gtym0"
      },
      "source": [
        "train_mnds = tf.data.Dataset.from_tensor_slices(\n",
        "    (image_train, label_train))\n",
        "test_mnds = tf.data.Dataset.from_tensor_slices(\n",
        "    (image_test, label_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAIw8RM3uZWW"
      },
      "source": [
        "Inspect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wouuHA52uYjm"
      },
      "source": [
        "train_mnds, test_mnds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkV4cPyfP_SJ"
      },
      "source": [
        "## Build Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHeEEeh8uiuw"
      },
      "source": [
        "Build pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjaTyjsaipvr"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "SHUFFLE_SIZE = 10000\n",
        "\n",
        "train_mnist = train_mnds.shuffle(SHUFFLE_SIZE).\\\n",
        "                         batch(BATCH_SIZE).prefetch(1)\n",
        "test_mnist = train_mnds.batch(BATCH_SIZE).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv8u_R6d2LM4"
      },
      "source": [
        "Inspect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXlQKWFVuxeE"
      },
      "source": [
        "train_mnist, test_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u7EsX5Uu36J"
      },
      "source": [
        "## Build Model\n",
        "\n",
        "Earlier, we imported requisite libraries. Since they are already in memory, we don't need to import them again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIhovfKKA8iN"
      },
      "source": [
        "Get tensor shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ1Y0YGp2Rhc"
      },
      "source": [
        "np_shape = image_test.shape[1:]\n",
        "np_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17nRzcVv8c-R"
      },
      "source": [
        "Clear previous models and generate seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTxJugnU8gJK"
      },
      "source": [
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qtX_QXM8kTI"
      },
      "source": [
        "Create model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klVQniCgvAc9"
      },
      "source": [
        "model = Sequential([\n",
        "  Flatten(input_shape=np_shape),\n",
        "  Dense(512, activation='relu'),\n",
        "  Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ_0d2HNQXfR"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9g9JdtOz-BD"
      },
      "source": [
        "Compile with sparse categorical crossentropy. Notice that we don't set **from_logits=True** because we use **softmax** activation to produce a probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL5NADfUvDxU"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX3aOM1tQZfI"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN3IlAMv0TWc"
      },
      "source": [
        "Train for three epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2H55XuSvJh7"
      },
      "source": [
        "epochs = 3\n",
        "history = model.fit(train_mnist, epochs=epochs, verbose=1,\n",
        "                     validation_data=test_mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3dqVXUVxEUr"
      },
      "source": [
        "Resources:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/load_data/numpy\n",
        "\n",
        "https://www.tensorflow.org/tutorials/quickstart/beginner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSKRjAyvgPOH"
      },
      "source": [
        "# Process Flowers Data with a Keras Utility\n",
        "\n",
        "The **Flowers** dataset is a public one that contains thousands of flower photos distributed into five classes. \n",
        "\n",
        "For a nice resource on processing flowers data:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/load_data/images\n",
        "\n",
        "For a nice resource on Keras preprocessing, peruse:\n",
        "\n",
        "https://keras.io/api/preprocessing/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_63db0gJFp3"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecdZ0mLN038W"
      },
      "source": [
        "Import the Image package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6-XiahCJFxX"
      },
      "source": [
        "import PIL.Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfeM8_zmHSvO"
      },
      "source": [
        "## Download Flowers Dataset\n",
        "\n",
        "The dataset contains several thousand photos of flowers in five sub-directories with one flower photo per class. The directory structure is as follows:\n",
        "\n",
        "flowers_photos/\n",
        "  daisy/\n",
        "  dandelion/\n",
        "  roses/\n",
        "  sunflowers/\n",
        "  tulips/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTb1aEzfHsjt"
      },
      "source": [
        "Download data with **tf.keras.utils.get_file** utility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQAeWBJmyCXB"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "url1 = 'https://storage.googleapis.com/download.tensorflow.org/'\n",
        "url2 = 'example_images/flower_photos.tgz'\n",
        "dataset_url = url1 + url2\n",
        "\n",
        "data_dir = tf.keras.utils.get_file(origin=dataset_url, \n",
        "                                   fname='flower_photos', \n",
        "                                   untar=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywW2k1nq_PEK"
      },
      "source": [
        "**tf.keras.utils.get_file** downloads a file from a URL if it not already in the cache. **pathlib.Path** provides a concrete path to the files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6BWbTffIqjl"
      },
      "source": [
        "Grab the number of flower photos available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRAon5NfIqpb"
      },
      "source": [
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print (image_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxpDNa391rrg"
      },
      "source": [
        "There are available 3670 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcvY1Vv_-q6Q"
      },
      "source": [
        "The **data_dir** path points to directories that each hold a different type of flower. Let's see the directories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0lYd8-w-rAk"
      },
      "source": [
        "dirs = [item.name for item in data_dir.glob('*')\\\n",
        "        if item.name != 'LICENSE.txt']\n",
        "dirs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfqRd0pqtR4K"
      },
      "source": [
        "Access some of the files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFr01px8tR_J"
      },
      "source": [
        "files = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
        "fn = []\n",
        "for f in files.take(4):\n",
        "  print(f.numpy()), fn.append(str(f.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfjKUbvoyJG4"
      },
      "source": [
        "Display labels from each file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjswWENuvI79"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "label = []\n",
        "for i in range(4):\n",
        "  parts = Path(fn[i]).parts\n",
        "  label.append(parts[5])\n",
        "  print (parts[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWfmwprNI5LF"
      },
      "source": [
        "Each directory contains images of that type of flower. Here is the first flower in the daisy directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQTA8RUGI5Rb"
      },
      "source": [
        "daisy = list(data_dir.glob('daisy/*'))\n",
        "parts = Path(daisy[0]).parts\n",
        "x = ' '\n",
        "print (25 * x, parts[5])\n",
        "PIL.Image.open(str(daisy[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXT-xfGsLAWi"
      },
      "source": [
        "Number of daisy images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t1HjM3lK5Lp"
      },
      "source": [
        "len(daisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iorzmojI1-IQ"
      },
      "source": [
        "###Display Several Flower Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axhMf6pa3tov"
      },
      "source": [
        "Get flower type:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB-laGNY3nU-"
      },
      "source": [
        "roses = list(data_dir.glob('roses/*'))\n",
        "str(roses[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxHoGzcM0Dc6"
      },
      "source": [
        "Grab labels from some of the files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tlKc6_50DjP"
      },
      "source": [
        "label = []\n",
        "for i in range(4):\n",
        "  tup = Path(str(roses[i])).parts\n",
        "  label.append(tup[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSPJNuYA34tI"
      },
      "source": [
        "Display:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty03B76Q1-Nw"
      },
      "source": [
        "rows, cols = 2, 2\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(rows*cols):\n",
        "  plt.subplot(rows, cols, i + 1)\n",
        "  pix = np.array(PIL.Image.open(str(roses[i])))\n",
        "  plt.imshow(pix)\n",
        "  plt.title(label[i])\n",
        "  plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cssrLV19xDpm"
      },
      "source": [
        "Notice that images are not of the same size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNlXkshuL9H5"
      },
      "source": [
        "## Load using tf.keras.preprocessing\n",
        "\n",
        "Load images off disk using image_dataset_from_directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX1Imo5KH0-S"
      },
      "source": [
        "### Set Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-huEen054dV5"
      },
      "source": [
        "Set batch size, image height, and image width:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G36OVDLlgEQh"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMOe9koD4iVA"
      },
      "source": [
        "We can tell from the displayed roses that image size differs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urJ1k9uiH80G"
      },
      "source": [
        "### Create Train and Test Sets\n",
        "\n",
        "The **tf.keras.preprocessing.image_dataset_from_directory** generates a *tf.data.Dataset* from image files in a directory. The utility allows us to conveniently split data, seed, resize, and batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krn_GtBCZ76U"
      },
      "source": [
        "Split data 80/20:\n",
        "\n",
        "The combination of the **validation_split** and **subset** parameters determines the train and test split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehZqwIQO5HGU"
      },
      "source": [
        "Set aside 80% for train data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhLLQxYkgEV-"
      },
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset='training',\n",
        "  seed=0,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzXkt9zh5Mb8"
      },
      "source": [
        "Set aside 20% for test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1_m44TQgEZA"
      },
      "source": [
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset='validation',\n",
        "  seed=0,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvaGKi_LjjGS"
      },
      "source": [
        "### Inspect Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffcD0zn5j4DR"
      },
      "source": [
        "Display tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6YBc99FiskJ"
      },
      "source": [
        "train_ds, test_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVGzrmUIj8pR"
      },
      "source": [
        "Grab the first batch from the train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2ZIdPUmjnsZ"
      },
      "source": [
        "for img, lbl in train_ds.take(1):\n",
        "  print (img.shape, lbl.shape)\n",
        "flower_shape, just_img = img.shape[1:],\\\n",
        "                         img.shape[1:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_sSj7sIkMYB"
      },
      "source": [
        "As expected, batch size is 32 and images are shape 180 x 180 x 3. The **3** value indicates that images have three channels, which means they are RGB (color). Labels have shape (32,) that correspond to the 32 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCatEyOCIbOP"
      },
      "source": [
        "## Get Class Names\n",
        "\n",
        "We already identified the directory names. But, we can also access them with the **class_names** method from the utility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUXecImEgEbr"
      },
      "source": [
        "class_names = train_ds.class_names\n",
        "class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbSXgqAkIfAZ"
      },
      "source": [
        "## Display Examples\n",
        "\n",
        "Grab the first batch from the train set and plot some images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MuEIdHYgEe0"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype('uint8'))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxwwUIz5NIka"
      },
      "source": [
        "## Scale the Data\n",
        "\n",
        "As noted earlier, a pixel is represented by 256 values. So, RGB channel values are in the [0, 255] range. Since neural networks work better with small values, data is typically scaled to be in the [0, 1] range. Create a function to scale image data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvP6tZg3SivM"
      },
      "source": [
        "def format_image(image, label):\n",
        "  image = tf.image.resize(image, just_img) / 255.0\n",
        "  return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQDzxL9fOZIk"
      },
      "source": [
        "## Configure the Dataset for Performance\n",
        "\n",
        "We use buffered prefetching to get data from disk without having I/O issues. We cache to keep images in memory after they're loaded off disk during the first epoch, which ensures that the dataset does not become a bottleneck while training your model. As a result, caching save some operations (like file opening and data reading) from being executed during each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YhuqPYyO-QU"
      },
      "source": [
        "## Build the Input Pipeline\n",
        "\n",
        "Scale, shuffle, cache, and prefetch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDxCGZnDSqOs"
      },
      "source": [
        "SHUFFLE_SIZE = 100\n",
        "\n",
        "train_fds = train_ds.map(format_image).\\\n",
        "  shuffle(SHUFFLE_SIZE).cache().prefetch(1)\n",
        "test_fds = test_ds.map(format_image).\\\n",
        "  cache().prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4ZFUtRyddMO"
      },
      "source": [
        "Inspect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIPolYCiddSD"
      },
      "source": [
        "train_fds, test_fds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK62ktpZPKOT"
      },
      "source": [
        "## Train a Model\n",
        "\n",
        "For completeness, we show how to train a simple model using the datasets just prepared. The model has not been tuned in any way. The goal is to show the mechanics using the datasets just created.\n",
        "\n",
        "Resource:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/images/classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYhRoVT51HY1"
      },
      "source": [
        "### Import New Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1isC_EX5lMV"
      },
      "source": [
        "We need additional libraries to build a Convolutional Neural Network (CNN) model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj0J7bd91Hdk"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3g96zzUF1uk"
      },
      "source": [
        "### Get Number of Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svwuth-s51ys"
      },
      "source": [
        "Hold the number of classes in a variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxSLUUzIF2M7"
      },
      "source": [
        "num_classes = len(class_names)\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndc5aqvZF2Cz"
      },
      "source": [
        "### Clear Previous Models and Generate Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ-UkTNH58ds"
      },
      "source": [
        "Clear any previous models and generate a random seed. We use zero for the seed, but any number can be substituted:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDypmX2OF2e7"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoQurP_XPqG2"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbhpMX-T6HP_"
      },
      "source": [
        "Create a multilayer CNN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwHucVF7S0p7"
      },
      "source": [
        "flower_model = tf.keras.Sequential([\n",
        "  Conv2D(32, 3, activation='relu',\n",
        "         input_shape=flower_shape),\n",
        "  MaxPooling2D(),\n",
        "  Conv2D(32, 3, activation='relu'),\n",
        "  MaxPooling2D(),\n",
        "  Conv2D(32, 3, activation='relu'),\n",
        "  MaxPooling2D(),\n",
        "  Flatten(),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-kokNQ5GZIQ"
      },
      "source": [
        "The first layer scales the data. The second layer contains 32 neurons with a 3 x 3 convolutional kernel (or filter). Activation is **relu**. The third layer uses maximum pooling to reduce the spatial size of a layer by just keeping the maximum values. As such, the pooling layer reduces image dimensionality without losing important features or patterns. The next four layers repeat the same pattern as the second and third layers. The flatten layer converts pooled data into a single column because a Dense layer expects data in this form. The final Dense layer enables classification and prediction. Outputs are normalized because softmax activation is applied to them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv1bv4jxPuQ2"
      },
      "source": [
        "### Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYUJNV4U9MTn"
      },
      "source": [
        "Compile with SparseCategoricalCrossentropy() since softmax is applied to outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLV_W2lUWX95"
      },
      "source": [
        "flower_model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHHoWiVsPxuU"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8rRi8lr9S1P"
      },
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBa4djFDjSMl"
      },
      "source": [
        "history = flower_model.fit(\n",
        "    train_fds,\n",
        "    validation_data=test_fds,\n",
        "    epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvGGpBGAQC22"
      },
      "source": [
        "The model is overfitting because validation accuracy is low to the compared to the training accuracy.\n",
        "\n",
        "For an excellent resource on mitigating overfitting, peruse:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/overfit_and_underfit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9gFdfMFf0mh"
      },
      "source": [
        "# Process Flowers from Google Cloud Storage\n",
        "\n",
        "As we already know, the flowers dataset is organized into 5 folders. Each folder contains flowers of one kind. The folders are named sunflowers, daisy, dandelion, tulips and roses. The data is hosted in a public bucket on Google Cloud Storage (GCS).\n",
        "\n",
        "We can read flowers data as JPEG files or as TFRecord files. We begin by reading flowers as JPEG files and performing simple processing. We continue by presenting a complete training example with TFRecord files. Modeling TFRecord files is much more efficient than modeling JPEG files. For optimal performance, we read from multiple TFRecord files at once.\n",
        "\n",
        "The **TFRecord format** is a simple format for storing a sequence of binary records. Tensorflow's preferred file format for storing data is the protobuf-based TFRecord format. A TFRecord file contains a sequence of records, which can only be read sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfZpictAsddD"
      },
      "source": [
        "## Read Flowers as JPEG Files and Perform Simple Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSsu7A0moOHl"
      },
      "source": [
        "Read file names based on a GCS pattern:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT3S08bxwid-"
      },
      "source": [
        "GCS_PATTERN = 'gs://flowers-public/*/*.jpg'\n",
        "filenames = tf.io.gfile.glob(GCS_PATTERN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAKSnm0ywijk"
      },
      "source": [
        "GCS_PATTERN is a *glob pattern* that supports the * and ? wildcards. **Globs** (also known as glob patterns) are patterns that can expand a wildcard pattern into a list of pathnames that match the given pattern.\n",
        "\n",
        "For more information on globs, peruse:\n",
        "\n",
        "https://www.malikbrowne.com/blog/a-beginners-guide-glob-patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3-S30bJw9Tm"
      },
      "source": [
        "Grab JPEG images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbjpFAvZwiqJ"
      },
      "source": [
        "num_images = len(filenames)\n",
        "print ('Pattern matches {} images.'.format(num_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuGJKGwpwiur"
      },
      "source": [
        "Create a dataset of filenames from GCS_PATTERN and peruse its contents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXEWMOCwwi2i"
      },
      "source": [
        "filenames_ds = tf.data.Dataset.list_files(GCS_PATTERN)\n",
        "for filename in filenames_ds.take(5):\n",
        "  print (filename.numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip0x6fzNwi8Y"
      },
      "source": [
        "Create a function that returns a dataset of (image, label) tuples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqimO3rZwjCu"
      },
      "source": [
        "def decode_jpeg_and_label(filename):\n",
        "  bits = tf.io.read_file(filename)\n",
        "  image = tf.image.decode_jpeg(bits)\n",
        "  label = tf.strings.split(\n",
        "      tf.expand_dims(filename, axis=-1), sep='/')\n",
        "  label = label.values[-2]\n",
        "  return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8ub9CnjwjHj"
      },
      "source": [
        "Map the function to filenames_ds to create a dataset of (image, label) tuples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOmUR6FgwjOQ"
      },
      "source": [
        "ds = filenames_ds.map(decode_jpeg_and_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMFZN5vywjS-"
      },
      "source": [
        "Peruse:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrS_7VMtwjaa"
      },
      "source": [
        "for image, label in ds.take(5):\n",
        "  print (image.numpy().shape,\n",
        "         label.numpy().decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzoeZqbX0LuC"
      },
      "source": [
        "Notice that the processing speed is pretty slow!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPpjoklO0UJu"
      },
      "source": [
        "Display an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw6VUjRC0L2t"
      },
      "source": [
        "for img, lbl  in ds.take(1):\n",
        "  plt.axis('off')\n",
        "  plt.title(lbl.numpy().decode('utf-8'))\n",
        "  fig = plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6-NSswWdldP"
      },
      "source": [
        "Although we don't model data with this dataset, let's see how to convert text labels to encoded labels: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wzGVmjIdllQ"
      },
      "source": [
        "for img, lbl  in ds.take(1):\n",
        "  label = lbl.numpy().decode('utf-8')\n",
        "\n",
        "matches = tf.stack([tf.equal(label, s)\\\n",
        "                    for s in class_names], axis=-1)\n",
        "one_hot = tf.cast(matches, tf.float32)\n",
        "print (matches.numpy(), one_hot.numpy())\n",
        "new_label = tf.math.argmax(one_hot) # transform from one-hot array to class number\n",
        "new_label.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6KfkdLifxgT"
      },
      "source": [
        "Take a label from the dataset. Compare the label against the class name list to find its position in the list. Create a one-hot vector. Finally, convert the one-hot vector into a label tensor. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAzCFf1W0L8j"
      },
      "source": [
        "## Read and Process Flowers as TFRecord Files\n",
        "\n",
        "For an excellent resource on processing TFRecord files, peruse:\n",
        "\n",
        "https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/04_Keras_Flowers_transfer_learning_playground.ipynb#scrollTo=9u3d4Z7uQsmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWlssWzc0qaD"
      },
      "source": [
        "Read TFRecord files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjdmJSIN0qgf"
      },
      "source": [
        "piece1 = 'gs://flowers-public/'\n",
        "piece2 = 'tfrecords-jpeg-192x192-2/*.tfrec'\n",
        "TFR_GCS_PATTERN = piece1 + piece2\n",
        "tfr_filenames = tf.io.gfile.glob(TFR_GCS_PATTERN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v2DnBNx6a-P"
      },
      "source": [
        "Get the number of buckets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMjJ3Q0f6bEN"
      },
      "source": [
        "num_images = len(tfr_filenames)\n",
        "print ('Pattern matches {} image buckets.'.format(num_images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-vpx17AB9Qq"
      },
      "source": [
        "We grabbed 16 buckets. Since there are 3670 flower files, 15 buckets contain 230 images and the final bucket contains 220 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQcqdJuUCo_d"
      },
      "source": [
        "Display a file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97RMMQaYB9Wy"
      },
      "source": [
        "filenames_tfrds = tf.data.Dataset.list_files(TFR_GCS_PATTERN)\n",
        "for filename in filenames_tfrds.take(1):\n",
        "  print (filename.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29WYuMsiJkPe"
      },
      "source": [
        "### Set Parameters for Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6yfR9jCJ4lP"
      },
      "source": [
        "Set parameters for image resizing, pipelining, and number of epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6e99hXPJknW"
      },
      "source": [
        "IMAGE_SIZE = [192, 192]\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_SIZE = 100\n",
        "EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmdrnsJbKKp3"
      },
      "source": [
        "Set parameters for data splits and labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIRgNe3nKKvf"
      },
      "source": [
        "VALIDATION_SPLIT = 0.19\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp3id3ywMV8o"
      },
      "source": [
        "Create data splits, validation steps, and steps per epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxdw2mIuMWDo"
      },
      "source": [
        "split = int(len(tfr_filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = tfr_filenames[split:]\n",
        "validation_filenames = tfr_filenames[:split]\n",
        "print ('Splitting dataset into {} training files and {} validation files'\\\n",
        "       .format(len(tfr_filenames), len(training_filenames),\n",
        "               len(validation_filenames)), end = ' ')\n",
        "print ('with a batch size of {}.'.format(BATCH_SIZE))\n",
        "\n",
        "validation_steps = int(3670 // len(tfr_filenames) *\\\n",
        "                       len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(tfr_filenames) *\\\n",
        "                      len(training_filenames)) // BATCH_SIZE\n",
        "print ('There are {} batches per training epoch and {} '\\\n",
        "       'batches per validation run.'\\\n",
        "       .format(BATCH_SIZE, steps_per_epoch, validation_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw-U6dJH0qmh"
      },
      "source": [
        "### Create Functions to Load and Process TFRecord Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld7Gmp-J3FQo"
      },
      "source": [
        "Create a function to parse a TFRecord file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl5gsASD0qsH"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "  features = {\n",
        "      'image': tf.io.FixedLenFeature([], tf.string),\n",
        "      'class': tf.io.FixedLenFeature([], tf.int64)\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  image = tf.image.decode_jpeg(example['image'], channels=3)\n",
        "  image = tf.cast(image, tf.float32) / 255.0 \n",
        "  image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
        "  class_label = example['class']\n",
        "  return image, class_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZsa3UqX0qws"
      },
      "source": [
        "The function accepts an example TFRecord file. A dictionary holds datatypes common to TFRecords. The tf.string data type converts the image to byte strings (list of bytes). The tf.int64 converts the class label to a 64-bit integer scalar value. The example is parsed into (image, label) tuples. The image element, a JPEG-encoded image, is decoded into a uint8 image tensor. The image tensor is scaled to \\[0, 1\\] range for faster training. It is then reshaped to a standard size for model consumption. The class label element is cast to a scalar. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDi3-Dzd3qbn"
      },
      "source": [
        "Create a function to load TFRecord files as tf.data.Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUgi-RPq3qhu"
      },
      "source": [
        "def load_dataset(filenames):\n",
        "  option_no_order = tf.data.Options()\n",
        "  option_no_order.experimental_deterministic = False\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      filenames, num_parallel_reads=AUTO)\n",
        "  dataset = dataset.with_options(option_no_order)\n",
        "  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6BtI_cLEpAq"
      },
      "source": [
        "The function accepts TFRecord files. For optimal performance, code is included to read from multiple TFRecord files at once. The options setting allows order-altering optimizations. As such, **n** files are read in parallel and data order is disregarded in favor of reading speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqW-gFIO3tpx"
      },
      "source": [
        "Create a function to build an input pipeline from TFRecord files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV5T9JbF3tvV"
      },
      "source": [
        "def get_batched_dataset(filenames, train=False):\n",
        "  dataset = load_dataset(filenames)\n",
        "  dataset = dataset.cache()\n",
        "  if train:\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(SHUFFLE_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(AUTO)\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrZYH9BcIApf"
      },
      "source": [
        "The function accepts TFRecord files and calls the `load_dataset` function. The function continues by building an input pipeline by caching, repeating, shuffling, batching and prefetching the dataset. Repeating and shuffling are only mapped to training data. We follow best practices by repeating and shuffling only the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAZGwNzEIEA9"
      },
      "source": [
        "### Create Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2miHsKI0q7U"
      },
      "source": [
        "Instantiate the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvLaalSe0rKo"
      },
      "source": [
        "training_dataset = get_batched_dataset(\n",
        "    training_filenames, train=True)\n",
        "validation_dataset = get_batched_dataset(\n",
        "    validation_filenames, train=False)\n",
        "training_dataset, validation_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya3TKj2N0rPh"
      },
      "source": [
        "Display an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIfAlqz80rVC"
      },
      "source": [
        "for img, lbl in training_dataset.take(1):\n",
        "  plt.axis('off')\n",
        "  plt.title(CLASSES[lbl[0].numpy()])\n",
        "  fig = plt.imshow(img[0])\n",
        "  tfr_flower_shape = img.shape[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOsBZ0clR6m2"
      },
      "source": [
        "### Model Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5hz4bR3f-1d"
      },
      "source": [
        "Clear and seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNyTm6SDlvQ6"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvKX3AaMR4sm"
      },
      "source": [
        "Create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1cy0rIKR4zd"
      },
      "source": [
        "tfr_model = Sequential([\n",
        "  Conv2D(32, (3, 3), activation = 'relu',\n",
        "         input_shape=tfr_flower_shape),\n",
        "  MaxPooling2D(2, 2),\n",
        "  Conv2D(64, (3, 3), activation='relu'),\n",
        "  MaxPooling2D(2, 2),\n",
        "  Conv2D(128, (3, 3), activation='relu'),\n",
        "  MaxPooling2D(2),\n",
        "  Conv2D(128, (3, 3), activation='relu'),\n",
        "  MaxPooling2D(2, 2),\n",
        "  Flatten(),\n",
        "  Dense(512, activation='relu'),\n",
        "  Dense(num_classes, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNylrJRR45e"
      },
      "source": [
        "Inspect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArKUslBRR5J0"
      },
      "source": [
        "tfr_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLKF0K1oShon"
      },
      "source": [
        "Compile:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk4SYqYnR4-8"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "\n",
        "tfr_model.compile(optimizer='adam',\n",
        "              loss=loss,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU1HeU3mR5E9"
      },
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6CrCwVrSSwT"
      },
      "source": [
        "history = tfr_model.fit(training_dataset, epochs=EPOCHS,\n",
        "                    verbose=1, steps_per_epoch=steps_per_epoch,\n",
        "                    validation_steps=validation_steps, \n",
        "                    validation_data=validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}