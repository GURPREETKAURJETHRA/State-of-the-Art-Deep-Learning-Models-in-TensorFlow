{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ch14.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0DIZQALOE23"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "Reinforcement learning is training machine learning models to make a sequence of decisions. A software **agent** makes **observations** and takes actions within an **environment** to receive rewards. Its objective is to learn to act in a way that maximizes its expected rewards over time.  \n",
        "\n",
        "The model employs trial and error to come up with a solution to the problem. It gets either rewards or penalties for the actions it performs with the goal to maximize the total reward. Although the designer sets the reward policy (the rules of the game), he/she gives the model no hints or suggestions for how to solve the game. The model must figure out how to perform the task to maximize the reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7TtAR5kxYoy"
      },
      "source": [
        "# Policy Search\n",
        "\n",
        "A **policy** is the algorithm a software agent uses to determine its actions. The **policy space** is a mapping from perceived states of the environment to actions to be taken when in those states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyHEcXv30Bhf"
      },
      "source": [
        "# OpenAI Gym\n",
        "\n",
        "We need a working environment to train an agent. *OpenAI Gym* is a toolkit that provides a wide variety of simulated environments including Atari games, board games, 2D and 3d physical simulations, and so on.\n",
        "\n",
        "Resources:\n",
        "\n",
        "https://colab.research.google.com/drive/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H\n",
        "\n",
        "https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t\n",
        "\n",
        "https://stackoverflow.com/questions/50107530/how-to-render-openai-gym-in-google-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKvaJr-1ycnw"
      },
      "source": [
        "# Install and Configure OpenAI Gym on Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7idcsVDny7vw"
      },
      "source": [
        "Most of the requirements of python packages are already fulfilled on CoLab. To run Gym, install prerequisites:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POuTao38ycwe"
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeq9tJI6zWPQ"
      },
      "source": [
        "For the rendering environment, use pyvirtualdisplay:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm_X9tCMzWxG"
      },
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioHCOdEcTX1X"
      },
      "source": [
        "# Import **tensorflow** library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpygsU5NTY4O"
      },
      "source": [
        "Import library and alias it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prmzt6Q0TdMM"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1PaVreZTcPm"
      },
      "source": [
        "# GPU Hardware Accelerator\n",
        "\n",
        "To vastly speed up processing, we can use the GPU available from the Google Colab cloud service. Colab provides a free Tesla K80 GPU of about 12 GB. Itâ€™s very easy to enable the GPU in a Colab notebook:\n",
        "\n",
        "1.\tclick **Runtime** in the top left menu\n",
        "2.\tclick **Change runtime** type from the drop-down menu\n",
        "3.\tchoose **GPU** from the Hardware accelerator drop-down menu\n",
        "4.\tclick **SAVE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu9mIFhNTgeW"
      },
      "source": [
        "Verify that GPU is available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-LVWYafThG_"
      },
      "source": [
        "tf.__version__, tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcllIuuxVKfX"
      },
      "source": [
        "# Import Requisite Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5eRwZX4VN8e"
      },
      "source": [
        "To activate the virtual display:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64LS09veVKkf"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "display = pyvirtualdisplay.Display(\n",
        "    visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZPIX52b01YV"
      },
      "source": [
        "Import the **gym** library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITRgIvjo01ok"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXmVNSq125tR"
      },
      "source": [
        "# Create an Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPvqGtus1ftj"
      },
      "source": [
        "Create a **CartPole** environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBw199AS1mse"
      },
      "source": [
        "env = gym.make('CartPole-v1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W51giejf2Hd6"
      },
      "source": [
        "The *CartPole* environment is a 2D simulation that accelerates a cart left or right to balance a pole placed on top of it. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright and the goal is to prevent it from falling over."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7USxVMBS1qBU"
      },
      "source": [
        "Initialize the environment by calling is reset() method, which returns an observation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmamatDR1qNV"
      },
      "source": [
        "env.seed(0)\n",
        "obs = env.reset()\n",
        "obs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f__W2QpG18E6"
      },
      "source": [
        "Observations vary depending on the environment. In this case, it is a 1D numpy array composed of 4 floats that represent the cart's horizontal position, velocity, angle of the pole (0 = vertical), and angular velocity. Any positive number indicates movement to the **right** for angle of the pole and angular velocity. Any negative number indicates movement to the **left**. For horizontal position, a negative number means that it is tilting left and a positive number to the right. For velocity, a positive number means the cart is speeding up and a negative number slowing down.\n",
        "\n",
        "So the pole is not completely horizontal (obs[0] is slightly negative), its velocity is slowly increasing (obs[1] is slightly positive), the pole is angled slightly to the right (obs[2] is slightly positive), and the angular velocity is going toward the left (obs[3] is slightly negative)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlZ8s-jA2xVR"
      },
      "source": [
        "An environment can be visualized by calling its render() method, and you can pick the rendering mode (the rendering options depend on the environment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfSSdjOs2xn5"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_hK5kSb37LJ"
      },
      "source": [
        "# Display the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShgoLJx03I57"
      },
      "source": [
        "Set mode='rgb_array' to get an image of the environment as a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBklNNna2xqH"
      },
      "source": [
        "img = env.render(mode='rgb_array')\n",
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBPWSJGt3Tv-"
      },
      "source": [
        "Create a function to display the environment as configured:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04F-SfvO2weY"
      },
      "source": [
        "def plot_environment(env, figsize=(5,4)):\n",
        "  plt.figure(figsize=figsize)\n",
        "  img = env.render(mode='rgb_array')\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Tiiw9p4KZ9"
      },
      "source": [
        "Display:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaW5Ec0k3rsI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4FHO1NvlCKj"
      },
      "source": [
        "# Display Actions\n",
        "\n",
        "Let's see how to interact with the environment we created. The agent needs to select an action from an action space. An **action space** is the set of possible actions that an agent can take."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPWJrS7A3hhw"
      },
      "source": [
        "Ask the environment about possible actions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rHLLedC3hnY"
      },
      "source": [
        "env.action_space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAM4OypW4mM3"
      },
      "source": [
        "**Discrete(2)** means that the possible actions are integers 0 and 1, which represents accelerating left (0) or right (1). So the environment's action space has two possible actions. The agent can accelerate towards the left or towards the right. Of course, other environments may have additional discrete actions or other kinds of actions like continuous ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfjOQx6b5k_h"
      },
      "source": [
        "Reset the enviroment and see how the pole is leaning by looking at its angle:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fPLhxvb5lFn"
      },
      "source": [
        "env.seed(0)\n",
        "obs = env.reset()\n",
        "indx = 2\n",
        "obs[indx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpHkDKBI6PsM"
      },
      "source": [
        "The third position (index of 2) in the *obs* array is the angle of the pole. If the value is below 0, the pole angles to the left. If above 0, it angles to the right. The pole is moving slightly toward the right because **obs[2] is > 0**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZr3IKt05OE7"
      },
      "source": [
        "The CartPole environment only has two actions, left (0) or right (1). Let's accelerate the cart toward the right by setting **action=1**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWJzRa1K3h8L"
      },
      "source": [
        "action = 1\n",
        "obs, reward, done, info = env.step(action)\n",
        "print ('obs array:', obs)\n",
        "print ('reward:', reward)\n",
        "print ('done:', done)\n",
        "print ('info:', info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqMUcvC--WZ2"
      },
      "source": [
        "The **step()** method executes the given action and returns four values. **obs** is the new observation. The cart is now moving toward the right because **obs[1] > 0**. The pole is still tilted toward the right because **obs[2] > 0**, but its angular velocity is now negative because **obs[3] < 0**. So it will likely be tilted toward the left after the next step. In this simple environment, *reward* is always *1.0* at every step. So the goal is to keep the episode running as long as possible. The *done* value is *True* when the episode is over. The episode is over if the pole tilts too much, goes off the screen or we win the game. The *info* value provides extra information. Once we finish using an environment, call the **close()** method to free resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lygt0lKu8em7"
      },
      "source": [
        "The environment tells the agent each new observation, the reward, when the game is over, and information it got during the last step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLuQrUEI4W7j"
      },
      "source": [
        "Display the pole position:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKYEjDCV4XDh"
      },
      "source": [
        "plot_environment(env)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTI_Rppf8ahK"
      },
      "source": [
        "Here is the reward the agent got during the last step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjXiCaJ98am7"
      },
      "source": [
        "reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laPUdSGn8_4_"
      },
      "source": [
        "The game is not over yet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvEaZPSv8_-Z"
      },
      "source": [
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2E5BsJ79Fhl"
      },
      "source": [
        "The sequence of steps between the moment the environment is reset until it is done is called an **episode**. At the end of an episode (i.e., when step() returns done=True), reset the environment before continuing to use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Tl8ZCk9Fq_"
      },
      "source": [
        "if done:\n",
        "  obs = env.reset()\n",
        "else:\n",
        "  print ('game is not over!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAJ6T8rwlG_r"
      },
      "source": [
        "# Simple Neural Network Policy\n",
        "\n",
        "How can we make the poll remain upright? We need to define a policy, which is the strategy that the agent uses to select an action at each step. It can use all past actions and observations to decide what to do.\n",
        "\n",
        "Let's create a neural network that takes observations as inputs and output the action to take for each observation. To choose an action, the network estimates a probability for each action and selects an action randomly according to the estimated probabilities. In the case of the Cart-Pole environment, there are just two possible actions (left or right). So we only need one output neuron that outputs the probability *p* of the action 0 (left), and of course the probability of action 1 (right) will be *1 - p*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsDHqwFGume8"
      },
      "source": [
        "Clear previous models and generate a seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRzfuloFch98"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reGJMdgl-9lB"
      },
      "source": [
        "Determine the observation space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_SDjROO-9qx"
      },
      "source": [
        "obs_space = env.observation_space.shape\n",
        "obs_space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIUGfNjUAhOQ"
      },
      "source": [
        "As shown earlier in this notebook, the observation (or policy) space is a 1D numpy array composed of 4 floats that represent the cart's horizontal position, velocity, angle of the pole (0 = vertical), and angular velocity. So the policy space is 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scad5XmbAG1o"
      },
      "source": [
        "Set the number of inputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp1JFv9uAHEH"
      },
      "source": [
        "n_inputs = env.observation_space.shape[0]\n",
        "n_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzF54s8ZBLoi"
      },
      "source": [
        "Create a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtQY2wezBLx5"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "  Dense(5, activation='elu', input_shape=[n_inputs]),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGh5pcfvxZDv"
      },
      "source": [
        "The model is a simple *Sequential* model that defines the policy network. The number of inputs is the size of the observation space, which is 4 in our case. We include only 5 neurons in the first layer because this is such a simple problem. We output a single probability (the probability of going left) so we have a single output neuron using sigmoid activation. If we had more than two possible actions, we would use one output neuron per action and substitute softmax activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzNjnJ3lBTZF"
      },
      "source": [
        "In this particular environment, past actions and observations can safely be ignored because each observation contains the environment's full state. If there were some hidden state, we may need to consider past actions and observations to try to infer the hidden state of the environment. For example, if the environment only revealed the position of the cart but not its velocity, we have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is if the observations are noisy, we may want to use the past few observations to estimate the most likely current state. Our problem is as simple as can be because the current observation is noise-free and contains the environment's full state.\n",
        "\n",
        "Why do we pick a random action based on the probability given by the policy network rather than just picking the action with the highest probability? This approach lets the agent find the right balance between exploring new actions and exploiting the actions that are known to work well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfttE66yDx3n"
      },
      "source": [
        "# Model Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT2VWBh9C_pj"
      },
      "source": [
        "Create a function that runs the model to play one episode and return the frames so we can display an animation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOGOvhbwDAP6"
      },
      "source": [
        "def render_policy_net(model, n_max_steps=200, seed=0):\n",
        "  frames = []\n",
        "  env = gym.make('CartPole-v1')\n",
        "  env.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  obs = env.reset()\n",
        "  for step in range(n_max_steps):\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    left_proba = model.predict(obs.reshape(1, -1))\n",
        "    action = int(np.random.rand() > left_proba)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "      break\n",
        "  env.close()\n",
        "  return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7bGd7QFDAZ5"
      },
      "source": [
        "Establish the environment and reset it. Create a loop to run a number of steps until the episode is over. Begin each step by appending the visualization of the environment to the *frames* list. Continue making action predictions with the model. Next, establish an action based on prediction. Execute the *step()* method based on the action. Continue looping until the episode is over. End by returning the list of frames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgDPjvJQGOA5"
      },
      "source": [
        "Create functions to show animation of the frames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBQIf2HOGOHQ"
      },
      "source": [
        "import matplotlib.animation as animation\n",
        "import matplotlib as mpl\n",
        "\n",
        "def update_scene(num, frames, patch):\n",
        "  patch.set_data(frames[num])\n",
        "  return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "  fig = plt.figure()\n",
        "  patch = plt.imshow(frames[0])\n",
        "  plt.axis('off')\n",
        "  anim = animation.FuncAnimation(\n",
        "      fig, update_scene, fargs=(frames, patch), blit=True,\n",
        "      frames=len(frames), repeat=repeat, interval=interval)\n",
        "  plt.close()\n",
        "  return anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFHd7hL3NQNX"
      },
      "source": [
        "Model predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LKp7JecDAf5"
      },
      "source": [
        "frames = render_policy_net(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp_0qgYDNpNc"
      },
      "source": [
        "# Animate\n",
        "\n",
        "Additional resources:\n",
        "\n",
        "https://colab.research.google.com/github/jckantor/CBE30338/blob/master/docs/A.03-Animation-in-Jupyter-Notebooks.ipynb\n",
        "\n",
        "https://colab.research.google.com/github/phoebe-project/phoebe2-docs/blob/2.1/tutorials/animations.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJduad0gNhjk"
      },
      "source": [
        "Create the animation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMySFUOQNkN9"
      },
      "source": [
        "anim = plot_animation(frames, interval=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf6KzHroRZL3"
      },
      "source": [
        "Experiment with the **interval** parameter size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qio2krN7NuGk"
      },
      "source": [
        "Render and display the animation. We show two ways to accomplish this. The first method uses the HTML library to display HTML elements. The animation is rendered to html5 video with the to_html5_video() function and then displayed with HTML():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qQBctubHgDr"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "method1 = HTML(anim.to_html5_video())\n",
        "method1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWFZ2GnYPMel"
      },
      "source": [
        "The second method uses the runtime configuration library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICp6akcSIBRN"
      },
      "source": [
        "from matplotlib import rc\n",
        "\n",
        "method2 = rc('animation', html='html5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fgGmae8Q8Vu"
      },
      "source": [
        "Run the animation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxH4f32GIBTb"
      },
      "source": [
        "anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQsn8wjOze5b"
      },
      "source": [
        "Ugh! The pole is falling to the left! But, we didn't implement our basic policy to go left if the pole is tilting left and go right if it is tilting right.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXmO464g3oSP"
      },
      "source": [
        "# Implement a Basic Policy\n",
        "\n",
        "Make the network play in 50 different environments in parallel to give us a diverse training batch at each step. Train for 5,000 iterations. Use the RMSProp optimizer. And, use binary cross-entropy for the loss function because we only have two discrete possible actions. Finally, reset the environments when they are done to free resources. We train with a custom training loop so we can easily use the predictions at each training step to advance the environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycCLwZNG4INQ"
      },
      "source": [
        "Train with a basic policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqNtwd6c4ITJ"
      },
      "source": [
        "n_environments = 50\n",
        "n_iterations = 5000\n",
        "\n",
        "envs = [gym.make(\n",
        "    'CartPole-v1') for _ in range(n_environments)]\n",
        "for index, env in enumerate(envs):\n",
        "  env.seed(index)\n",
        "np.random.seed(0)\n",
        "observations = [env.reset() for env in envs]\n",
        "optimizer = tf.keras.optimizers.RMSprop()\n",
        "loss_fn = tf.keras.losses.binary_crossentropy\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "  # if angle < 0, we want proba(left) = 1., or else proba(left) = 0.\n",
        "  target_probas = np.array(\n",
        "      [([1.] if obs[2] < 0 else [0.]) \n",
        "      for obs in observations])\n",
        "  with tf.GradientTape() as tape:\n",
        "    left_probas = model(np.array(observations))\n",
        "    loss = tf.reduce_mean(\n",
        "        loss_fn(target_probas, left_probas))\n",
        "    print('\\rIteration: {}, Loss: {:.3f}'.\\\n",
        "          format(iteration, loss.numpy()), end='')\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(\n",
        "        zip(grads, model.trainable_variables))\n",
        "    actions = (np.random.rand(n_environments, 1) >\\\n",
        "               left_probas.numpy()).astype(np.int32)\n",
        "  for env_index, env in enumerate(envs):\n",
        "    obs, reward, done, info = env.step(\n",
        "        actions[env_index][0])\n",
        "    observations[env_index] = obs if not done else env.reset()\n",
        "\n",
        "for env in envs:\n",
        "  env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4lPmolg4IYI"
      },
      "source": [
        "Create the frames for an animation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDbhNVGV4IdM"
      },
      "source": [
        "frames = render_policy_net(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yhP-8aO4Ii7"
      },
      "source": [
        "Animate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T58qIy4m4Iny"
      },
      "source": [
        "anim = plot_animation(frames, repeat=True, interval=100)\n",
        "anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df9WOAce8KbJ"
      },
      "source": [
        "Much better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3_cmcNj8RSO"
      },
      "source": [
        "# Reinforce Algorithm\n",
        "\n",
        "Let's see if the agent can learn a better policy on its own. Policy gradients (PG) optimize the parameters of a policy by following the gradients toward higher rewards. We use a reinforce PG algorithm to automate agent learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_xfFqrP9Zsi"
      },
      "source": [
        "Let the neural network policy play the game several times. At each step, compute the gradients that make the chosen action even more likely. But, don't apply the gradients yet. After running several episodes, compute each action's advantage with a discount factor at each step. A *discount factor* is computed by evaluating an action based on the sum of all rewards that come after the action. If an action's advantage is positive, the action was probably good. So apply the gradients to make the action more likely to be chosen in the future. If it is negative, apply the opposite gradients to make the action less likely to be chosen. Finally, compute the mean of all resultant gradient vectors (gradient (or opposite gradient) x action advantage) and use it to perform a *Gradient Descent* step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrL_XCgV_12G"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Train the model to learn to balance the pole on the cart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7eDd1XMJSKI"
      },
      "source": [
        " ## Create Functions to Play the Game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9KbJ0RbJQt4"
      },
      "source": [
        "Create a function that plays one step: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wptcEIqRze_a"
      },
      "source": [
        "def play_one_step(env, obs, model, loss_fn):\n",
        "  with tf.GradientTape() as tape:\n",
        "    left_proba = model(obs[np.newaxis])\n",
        "    action = (tf.random.uniform([1, 1]) > left_proba)\n",
        "    y_target = tf.constant(\n",
        "        [[1.]]) - tf.cast(action, tf.float32)\n",
        "    loss = tf.reduce_mean(loss_fn(\n",
        "        y_target, left_proba))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  obs, reward, done, info = env.step(\n",
        "      int(action[0, 0].numpy()))\n",
        "  return obs, reward, done, grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cov705LpATYH"
      },
      "source": [
        "With the *GradientTape* block, call the model with a single observation. We reshape the observation so that it becomes a batch containing a single instance (the model expects a batch). We get a probability of going left. Sample a random float between 0 and 1, and check if it greater than the probability. The *action* is **False** with probability *left_proba* and **True** with probability *1 - left_proba*. Cast this Boolean to a number of 0 (left) or 1 (right) with the appropriate probabilities. We then define the target probabilities of going left (1 - the action). If the action is 0 (left), the target probability of going left is 1. If the action is 1 (right), the target probability is 0. Whew!\n",
        "\n",
        "We continue by computing the loss and use the tape to compute the gradient of the loss with regard to the model's trainable variables. We tweak the gradients later depending on how good or bad that action turned out to be. Finally, we play the selected action and return the new observation, reward, whether the episode is over or not, and the gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAKlb6_nCiSY"
      },
      "source": [
        "Create a function to play multiple episodes and return the rewards and gradients for each episode and each step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmwyhWLNzrwy"
      },
      "source": [
        "def play_multiple_episodes(\n",
        "    env, n_episodes, n_max_steps, model, loss_fn):\n",
        "  all_rewards = []\n",
        "  all_grads = []\n",
        "  for episode in range(n_episodes):\n",
        "    current_rewards = []\n",
        "    current_grads = []\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "      obs, reward, done, grads = play_one_step(\n",
        "          env, obs, model, loss_fn)\n",
        "      current_rewards.append(reward)\n",
        "      current_grads.append(grads)\n",
        "      if done:\n",
        "        break\n",
        "    all_rewards.append(current_rewards)\n",
        "    all_grads.append(current_grads)\n",
        "  return all_rewards, all_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1fCOhBVDD9E"
      },
      "source": [
        "The function returns a list of reward lists. The list contains one reward list per episode. Each reward list contains one reward per step. It also returns a list of gradient lists. The list contains one gradient list per episode. Each gradient list contains one tuple of gradients per step. Each tuple contains one gradient tensor per trainable variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqqnQhgnEKhk"
      },
      "source": [
        "The policy gradient algorithm uses the *play_multiple_episodes()* function to play the game several times. It then goes back and looks at all the rewards to discount and normalize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy_sFGOgJg0x"
      },
      "source": [
        "## Discount and Normalize the Rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF-qeOVdJg4D"
      },
      "source": [
        "To discount and normalize rewards, we introduce two functions: *discount_rewards* and *discount_and_normalize_rewards*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh9uVqUEzrzR"
      },
      "source": [
        "def discount_rewards(rewards, discount_rate):\n",
        "  discounted = np.array(rewards)\n",
        "  for step in range(len(rewards) - 2, -1, -1):\n",
        "    discounted[step] += discounted[step + 1] * discount_rate\n",
        "  return discounted\n",
        "\n",
        "def discount_and_normalize_rewards(\n",
        "    all_rewards, discount_rate):\n",
        "  all_discounted_rewards =\\\n",
        "    [discount_rewards(rewards, discount_rate)\n",
        "    for rewards in all_rewards]\n",
        "  flat_rewards = np.concatenate(all_discounted_rewards)\n",
        "  reward_mean = flat_rewards.mean()\n",
        "  reward_std = flat_rewards.std()\n",
        "  return [(discounted_rewards - reward_mean) / reward_std\n",
        "          for discounted_rewards in all_discounted_rewards]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu-GdgSYFLk-"
      },
      "source": [
        "Verify that the function work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujCGqTonzr2G"
      },
      "source": [
        "discount_rewards([10, 0, -50], discount_rate=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4DSEjJxM8z8"
      },
      "source": [
        "We give the function 3 actions. After each action, there is a reward: first 10, then 0, then -50. We use a discount factor of 80%. So, the 3rd action gets -50 (full credit for the last reward), but the 2nd action only gets -40 (80% credit for the last reward), and the 1st action will get 80% of -40 (-32) plus full credit for the first reward (+10) leading to a discounted reward of -22."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGgfrJomNk1f"
      },
      "source": [
        "To normalize all discounted rewards across all episodes, we compute the mean and standard deviation of all the discounted rewards. We then subtract the mean from each discounted reward and divide by the standard deviation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhvZ0-b4zxv6"
      },
      "source": [
        "discount_and_normalize_rewards(\n",
        "    [[10, 0, -50], [10, 20]], discount_rate=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi4Sg8q6FPLd"
      },
      "source": [
        "All actions from the first episode are considered **bad** because normalized advantages are all negative. This makes sense because the sum of the rewards is -40. Conversely, second episode actions are **good** because normalized advantages are positive. The sum of the rewards is 30."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybgAgornJ6XW"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyuuFFXGIWeR"
      },
      "source": [
        "Define the hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPSKsnahzxyi"
      },
      "source": [
        "n_iterations = 150\n",
        "n_episodes_per_update = 10\n",
        "n_max_steps = 200\n",
        "discount_rate = 0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAcQD0SiIlcH"
      },
      "source": [
        "Run 150 training iterations, play 10 episodes of the game per iteration, make each episode last at most 200 steps, and use a discount rate of 0.95."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5RuRjF3I4UA"
      },
      "source": [
        "Define an optimizer and loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe_7h5Sszx1Q"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.binary_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgN3EJqbI-ZI"
      },
      "source": [
        "Use binary cross_entropy because we are training a binary classifier (two possible actions: left or right)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2nbszluKA-p"
      },
      "source": [
        "Create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB6nb9F0z_D9"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "model = Sequential([\n",
        "  Dense(5, activation='elu', input_shape=[4]),\n",
        "  Dense(1, activation='sigmoid'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAkxWb-xKFQc"
      },
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwjrIpCgz_GU"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(42);\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "  all_rewards, all_grads = play_multiple_episodes(\n",
        "      env, n_episodes_per_update, n_max_steps,\n",
        "      model, loss_fn)\n",
        "  total_rewards = sum(map(sum, all_rewards))\n",
        "  print('\\rIteration: {}, mean rewards: {:.1f}'.format(\n",
        "      iteration, total_rewards / n_episodes_per_update),\n",
        "      end='')\n",
        "  all_final_rewards = discount_and_normalize_rewards(\n",
        "      all_rewards, discount_rate)\n",
        "  all_mean_grads = []\n",
        "  for var_index in range(len(model.trainable_variables)):\n",
        "    mean_grads = tf.reduce_mean(\n",
        "        [final_reward * all_grads[episode_index][step][var_index]\n",
        "         for episode_index, final_rewards in enumerate(\n",
        "             all_final_rewards)\n",
        "           for step, final_reward in enumerate(\n",
        "               final_rewards)], axis=0)\n",
        "    all_mean_grads.append(mean_grads)\n",
        "  optimizer.apply_gradients(\n",
        "      zip(all_mean_grads, model.trainable_variables))\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0QOoVyKnC0"
      },
      "source": [
        "At each training iteration, call *play_multiple_episodes()*, which plays the game 10 times and returns all the rewards and gradients for every episode and step. Call *discount_and_normalize_rewards()* to compute each action's normalized advantage, which gives us a measure of how good or bad each action actually was in hindsight. For each trainable variable, we compute the weighted mean of the gradients over all episodes and all steps weighted by the *final_reward*. The **final_reward** is each action's normalized advantage. We end by applying the mean gradients using the optimizer, which tweaks the model's trainable variables to hopefully make the policy a bit better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm_GfCOIOF2t"
      },
      "source": [
        "## Render Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFKz61JMOLEV"
      },
      "source": [
        "Render the reinforce algorithm policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVZP4ilLOGBF"
      },
      "source": [
        "frames_ra = render_policy_net(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2IYh3smN5sg"
      },
      "source": [
        "## Animate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q58B0BXfOXCH"
      },
      "source": [
        "Animate the policy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3yrbs1Iz_I2"
      },
      "source": [
        "anim = plot_animation(frames_ra, repeat=True, interval=100)\n",
        "anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czFkl3al9MJa"
      },
      "source": [
        "A bit less wobbly."
      ]
    }
  ]
}