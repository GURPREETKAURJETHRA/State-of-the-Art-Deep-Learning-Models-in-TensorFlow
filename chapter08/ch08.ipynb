{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EXoS-8gwxlx"
      },
      "source": [
        "# Stacked Autoencoders\n",
        "\n",
        "**Autoencoders** are artificial neural networks that learn dense representations of the input data without any supervision. The dense representations are called *latent representations* or *codings*. The codings typically have much lower dimensionality than the input data, which makes autoencoders useful for dimensionality reduction. They can also act as feature detectors (feature extraction), unsupervised pretraining of deep neural networks, and generative models. As generative models, they can randomly generate new data that looks very similar to the training data.\n",
        "\n",
        "Simply, autoencoders are trained in an unsupervised manner to learn the low-level features of an input (latent representations or codings), which are then used to reconstruct the original input. So, an autoencoder consists of 3 components: encoder, latent representations (or codings), and decoder. The encoder compresses the input and produces the codings. The decoder then reconstructs the input from the codings.\n",
        "\n",
        "Resources:\n",
        "\n",
        "https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n",
        "\n",
        "https://www.tensorflow.org/tutorials/generative/autoencoder\n",
        "\n",
        "https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/autoencoder-keras-tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew6ILT-7Rbkc"
      },
      "source": [
        "# Import **tensorflow** library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_V_r2slQPxI"
      },
      "source": [
        "Import library and alias it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1B43zq5Rb2b"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7UC2JNPw3pY"
      },
      "source": [
        "# GPU Hardware Accelerator\n",
        "\n",
        "To vastly speed up processing, we can use the GPU available from the Google Colab cloud service. Colab provides a free Tesla K80 GPU of about 12 GB. Itâ€™s very easy to enable the GPU in a Colab notebook:\n",
        "\n",
        "1.\tclick **Runtime** in the top left menu\n",
        "2.\tclick **Change runtime** type from the drop-down menu\n",
        "3.\tchoose **GPU** from the Hardware accelerator drop-down menu\n",
        "4.\tclick **SAVE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDNFpWCTw7L4"
      },
      "source": [
        "Verify that GPU is active:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck2w8yEvwqpp"
      },
      "source": [
        "tf.__version__, tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu30Ldwuknh0"
      },
      "source": [
        "# Stacked Autoencoders\n",
        "\n",
        "**Stacked encoders** have multiple hidden layers. The architecture is typically symmetrical with regard to the central hidden layer, which is called the *coding layer*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YmINgYRlvZw"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "Load Fashion-Mnist as Numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApZDmWIDknms"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "(x_train_img, _), (x_test_img, _) = tfds.as_numpy(\n",
        "    tfds.load('fashion_mnist', split=['train','test'],\n",
        "              batch_size=-1, as_supervised=True,\n",
        "              try_gcs=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GCte2cfmWzA"
      },
      "source": [
        "Notice that we don't load the labels because autoencoders are unsupervised models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M6qeXremM6o"
      },
      "source": [
        "## Scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtnDyBlaQTsT"
      },
      "source": [
        "Scale by dividing datasets by the number of pixels that represent an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_2AvNWMmM_g"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train, x_test = x_train_img.astype(np.float32) / 255.,\\\n",
        "                  x_test_img.astype(np.float32) / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Vqfe3vkn0T"
      },
      "source": [
        "## Clear Previous Models and Generate Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKEPn9d0QdBf"
      },
      "source": [
        "Clear previous model sessions and generate a seed for reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJxvYwiNkn5k"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEgBWvh0mfqA"
      },
      "source": [
        "## Get Input Shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5MYhJRXQuNI"
      },
      "source": [
        "Get input shape for use in the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag2ff2YCmfuw"
      },
      "source": [
        "in_shape = x_train.shape[1:]\n",
        "in_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEp-duponITa"
      },
      "source": [
        "## Build Stacked Autoencoder\n",
        "\n",
        "Stacked encoders have multiple hidden layers. The architecture is typically symmetrical with regard to the central hidden layer, which is the coding layer. We split the autoencoder model into the encoder and decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcOoNB8gnLa4"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEc2w0SknIYE"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten,\\\n",
        "  Reshape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPUvM3ILNooj"
      },
      "source": [
        "In our example, the encoder accepts 28 x 28 pixel grayscale images, flattens them so that each image is represented as a vector of size 784, and processes the vectors through three Dense layers of diminishing sizes (128 units to 64 units to 32 units). The 32 unit layer is the coding layer (central hidden layer). For each input image, the encoder outputs a vector of size 32. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWBXI4mrnNbA"
      },
      "source": [
        "stacked_encoder = Sequential([\n",
        "  Flatten(input_shape=in_shape),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(32, activation='relu')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-PIKnrjCz1g"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8iLbmAMP5GF"
      },
      "source": [
        "The decoder accepts codings of size 32 (output by the encoder) and processes them through three Dense layers of increasing sizes (64 units to 128 units to 784 units). It then reshapes the final vectors into 28 x 28 arrays so the decoder's outputs have the same shape as the encoder's inputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hclEUvaGn3sy"
      },
      "source": [
        "stacked_decoder = Sequential([\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dense(28 * 28, activation='sigmoid'),\n",
        "  Reshape(in_shape)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liyEWDUvRJhp"
      },
      "source": [
        "## Create Stacked Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdIXWN52QxGo"
      },
      "source": [
        "Create stacked autoencoder based on stacked encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifu3mFoQRJn5"
      },
      "source": [
        "stacked_ae = Sequential([stacked_encoder, stacked_decoder])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeRzOzGPShGZ"
      },
      "source": [
        "## Create Appropriate Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpvOgfwKQ8h4"
      },
      "source": [
        "Create metric to track model performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJv88Ux4ShK-"
      },
      "source": [
        "def rounded_accuracy(y_true, y_pred):\n",
        "    return tf.keras.metrics.binary_accuracy(tf.round(y_true),\n",
        "                                            tf.round(y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_fljpRtS9cf"
      },
      "source": [
        "The *accuracy* metric won't work properly since it expects labels to be either 0 or 1 for each pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CTCiWJGRPRd"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4LNvTCKRHGI"
      },
      "source": [
        "Use **binary crossentropy** as the loss function because the reconstruction task is a multilabel binary classification problem since each pixel intensity represents the probability that the pixel should be black."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUBTFvjvRUYN"
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(lr=1.5)\n",
        "\n",
        "stacked_ae.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=opt, metrics=[rounded_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue8jUqULRPX8"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEpZXTyvRNVQ"
      },
      "source": [
        "Train the model using x_train as both the input and the target. The encoder will learn to compress the dataset from 784 dimensions to the latent space, and the decoder will learn to reconstruct the original images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewb2_9qfnZ7g"
      },
      "source": [
        "sae_history = stacked_ae.fit(\n",
        "    x_train, x_train, epochs=10,\n",
        "    validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_OVtA3CfBtP"
      },
      "source": [
        "## Visualize Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDiMBqXjgGMT"
      },
      "source": [
        "Import a plotting library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaNUbjaAgGTF"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6L_GZqWfPsA"
      },
      "source": [
        "Create a visualization function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw-J5VyifB9v"
      },
      "source": [
        "def viz_history(training_history):\n",
        "  loss = training_history.history['loss']\n",
        "  val_loss = training_history.history['val_loss']\n",
        "  accuracy = training_history.history['rounded_accuracy']\n",
        "  val_accuracy = training_history.history['val_rounded_accuracy']\n",
        "  plt.figure(figsize=(14, 4))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(loss, label='Training set')\n",
        "  plt.plot(val_loss, label='Test set', linestyle='--')\n",
        "  plt.legend()\n",
        "  plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(accuracy, label='Training set')\n",
        "  plt.plot(val_accuracy, label='Test set', linestyle='--')\n",
        "  plt.legend()\n",
        "  plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Eupv-GufWHQ"
      },
      "source": [
        "Visualize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbmjTPrWfWTr"
      },
      "source": [
        "viz_history(sae_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6-Si7hpKkj"
      },
      "source": [
        "## Visualize the Reconstructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHxfAA6YYdkO"
      },
      "source": [
        "Create a function to plot a grayscale 28x28 image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUbVPLwMnNiw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_image(image):\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQNfjeUoYl_V"
      },
      "source": [
        "Create a function to visualize original images and reconstructions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw0VMXZWYmD3"
      },
      "source": [
        "def show_reconstructions(model, images, n_images):\n",
        "  reconstructions = model.predict(images[:n_images])\n",
        "  reconstructions = tf.squeeze(reconstructions) # drop '1' dimension\n",
        "  fig = plt.figure(figsize=(n_images * 1.5, 3))\n",
        "  for image_index in range(n_images):\n",
        "    plt.subplot(2, n_images, 1 + image_index)\n",
        "    plot_image(images[image_index])\n",
        "    plt.subplot(2, n_images, 1 + n_images + image_index)\n",
        "    plot_image(reconstructions[image_index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivpkpPKorIK-"
      },
      "source": [
        "The predict() function adds the *1* dimension back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41eHnXvPXjQt"
      },
      "source": [
        "Check dimensionality of test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld6X1U0mXj2r"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM0YrgNUY1k8"
      },
      "source": [
        "To visualize with imshow(), we must remove dimensions of size 1 from the shape of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP7PY-F3W4Zn"
      },
      "source": [
        "x_test_imgs = tf.squeeze(x_test)\n",
        "x_test_imgs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66BnQMsgY9c8"
      },
      "source": [
        "Visualize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHU-BTaHY9hN"
      },
      "source": [
        "show_reconstructions(stacked_ae, x_test_imgs, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toLFhTAeVMVT"
      },
      "source": [
        "Reconstructed images are generated from **test images** based on predictions from the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzvvHELFQddh"
      },
      "source": [
        "## Breakdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQCqMJgJWaPh"
      },
      "source": [
        "Grab an image from the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7u4MV4wWaXf"
      },
      "source": [
        "img = x_test[:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ5ReIMgWhDn"
      },
      "source": [
        "Since the prediction method computations are done in batches, we grab the first image as a batch of one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV1zv9G7W3C4"
      },
      "source": [
        "Make a prediction based on the image batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtv5Om7mW3Jo"
      },
      "source": [
        "reconstruction = stacked_ae.predict(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGWFUAHWXA8x"
      },
      "source": [
        "Drop the '1' dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biRGfidGXBDo"
      },
      "source": [
        "reconstruction = tf.squeeze(reconstruction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnAMprQpXiJ6"
      },
      "source": [
        "Plot reconstruction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUuXMexZXiPi"
      },
      "source": [
        "plot_image(reconstruction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG_ijIv_X30i"
      },
      "source": [
        "Plot actual image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcHDU2BbX39O"
      },
      "source": [
        "plot_image(tf.squeeze(x_test[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXnnHOrYHh7"
      },
      "source": [
        "We squeeze out the '1' dimension from the image to plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cA-2bN3ZQfG"
      },
      "source": [
        "## Visualize with Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9nwWHBUbvtw"
      },
      "source": [
        "To perform dimensionality reduction, we need labels. So load labels from the **test data** set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmi0J281a1b_"
      },
      "source": [
        "test = tfds.as_numpy(\n",
        "    tfds.load('fashion_mnist', split=['test'],\n",
        "              batch_size=-1, as_supervised=True,\n",
        "              try_gcs=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6TjU308b6j5"
      },
      "source": [
        "Slice test labels from the test data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-13pGxGbBzu"
      },
      "source": [
        "y_test = test[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPPjWPuucBro"
      },
      "source": [
        "Use the encoder to reduce dimensionality to 32:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANm8WaY8ZWAu"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "np.random.seed(0)\n",
        "x_test_compressed = stacked_encoder.predict(x_test_imgs)\n",
        "tsne = TSNE()\n",
        "x_test_2D = tsne.fit_transform(x_test_compressed)\n",
        "x_test_2D = (x_test_2D - x_test_2D.min()) /\\\n",
        "  (x_test_2D.max() - x_test_2D.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwbwnD_fcnFc"
      },
      "source": [
        "We used Scikit-Learn's implementation of the t-SNE algorithm to reduce dimensionality to 2D for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDFda0egZWGn"
      },
      "source": [
        "Visualize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDty2umlZWL3"
      },
      "source": [
        "plt.scatter(x_test_2D[:, 0], x_test_2D[:, 1],\n",
        "            c=y_test, s=10, cmap='tab10')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ColxRICW3mcp"
      },
      "source": [
        "Each class is represented by a different color."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skDn5YtdZWQE"
      },
      "source": [
        "Display a prettier visualization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Waf-0o9sZWUU"
      },
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "cmap = plt.cm.tab10\n",
        "plt.scatter(x_test_2D[:, 0], x_test_2D[:, 1],\n",
        "            c=y_test, s=10, cmap=cmap)\n",
        "image_positions = np.array([[1., 1.]])\n",
        "for index, position in enumerate(x_test_2D):\n",
        "    dist = np.sum((position - image_positions) ** 2, axis=1)\n",
        "    if np.min(dist) > 0.02: # if far enough from other images\n",
        "        image_positions = np.r_[image_positions, [position]]\n",
        "        imagebox = mpl.offsetbox.AnnotationBbox(\n",
        "            mpl.offsetbox.OffsetImage(x_test_imgs[index],\n",
        "                                      cmap='binary'),\n",
        "            position, bboxprops={\n",
        "                'edgecolor': cmap(y_test[index]), 'lw': 2})\n",
        "        plt.gca().add_artist(imagebox)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KvNqRDdxQT"
      },
      "source": [
        "Adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81OIsQqwgW_E"
      },
      "source": [
        "# Tying Weights\n",
        "\n",
        "When an autoencoder is neatly symmetrical, we can tie the weights of the decoder layers to the weights of the encoder layers. As a result, we halve the number of weights in the model, which speeds training and reduces overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia_hCvPvg0rW"
      },
      "source": [
        "## Define a Custom Layer\n",
        "\n",
        "To tie the weights of the encoder and the decoder, we use the transpose of the encoder's weights as the decoder weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ezaYhKWg0ws"
      },
      "source": [
        "class DenseTranspose(tf.keras.layers.Layer):\n",
        "  def __init__(self, dense, activation=None, **kwargs):\n",
        "    self.dense = dense\n",
        "    self.activation = tf.keras.activations.get(activation)\n",
        "    super().__init__(**kwargs)\n",
        "  def build(self, batch_input_shape):\n",
        "    self.biases = self.add_weight(\n",
        "        name='bias', shape=[self.dense.input_shape[-1]],\n",
        "        initializer='zeros')\n",
        "    super().build(batch_input_shape)\n",
        "  def call(self, inputs):\n",
        "    z = tf.matmul(\n",
        "        inputs, self.dense.weights[0], transpose_b=True)\n",
        "    return self.activation(z + self.biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kJ76gtmh31c"
      },
      "source": [
        "The class accepts a layer from a model, an activation function (if included in a layer), and transposes the data. A lot of times we have to preprocess data fed into machine learning algorithms. The reason is that data may be stored as rows, but the machine learning algorithm expects input as columns or vice versa. So transposition is a very useful operation in machine learning.\n",
        "\n",
        "Resource:\n",
        "\n",
        "https://www.youtube.com/watch?v=QDpeRUIrb6U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvvJoUA5hmYF"
      },
      "source": [
        "## Clear Models and Generate Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDe1cgGrTWjL"
      },
      "source": [
        "Clear previous model sessions and generate a seed for reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEC-qA_nhtJZ"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EgEtc9EjR8e"
      },
      "source": [
        "## Create Dense Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G8IdolcTYH-"
      },
      "source": [
        "Create three dense layers for the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea_TPIb0hmek"
      },
      "source": [
        "dense_1 = Dense(128, activation='relu')\n",
        "dense_2 = Dense(64, activation='relu')\n",
        "dense_3 = Dense(32, activation='relu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2nWgj_XiqIX"
      },
      "source": [
        "## Build the Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTgCa35dTdOL"
      },
      "source": [
        "Build the encoder with three dense layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkFh3niCiqM-"
      },
      "source": [
        "tied_encoder = Sequential([\n",
        "  Flatten(input_shape=in_shape),\n",
        "  dense_1,\n",
        "  dense_2,\n",
        "  dense_3\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp2_HywNi-zf"
      },
      "source": [
        "## Build the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2afrMWGTk6J"
      },
      "source": [
        "Build the decoder and tie weights with the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfF5Aog8i-3w"
      },
      "source": [
        "tied_decoder = Sequential([\n",
        "  DenseTranspose(dense_3, activation='relu'),\n",
        "  DenseTranspose(dense_2, activation='relu'),\n",
        "  DenseTranspose(dense_1, activation='sigmoid'),\n",
        "  Reshape([28, 28])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmf6CEqthEQk"
      },
      "source": [
        "## Build Tied Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r55Rl20lT9cz"
      },
      "source": [
        "Build the model with tied weights between the encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzpPA3YngXDU"
      },
      "source": [
        "tied_ae = Sequential([tied_encoder, tied_decoder])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX57YLgHgXIE"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPOx6UuKUJBa"
      },
      "source": [
        "Compile with **binary crossentropy**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsWbEuuDgXMY"
      },
      "source": [
        "tied_ae.compile(loss='binary_crossentropy',\n",
        "                optimizer=opt, metrics=[rounded_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4McSoQXwgXQM"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG5slaILUNQ6"
      },
      "source": [
        "Train for ten epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqwtXHEdj1qh"
      },
      "source": [
        "tied_history = tied_ae.fit(\n",
        "    x_train, x_train, epochs=10,\n",
        "    validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1NMH9sOnUCI"
      },
      "source": [
        "Visualize training performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyzfeJeqnUKn"
      },
      "source": [
        "viz_history(tied_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVdclL6Ms7ma"
      },
      "source": [
        "## Visualize Reconstructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnAgL14hUToB"
      },
      "source": [
        "Show test image reconstructions based on predictions from the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrq46303s7x5"
      },
      "source": [
        "show_reconstructions(tied_ae, x_test_imgs, 6)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sgF6zHqtQRN"
      },
      "source": [
        "# Denoising Autoencoders\n",
        "\n",
        "An autoencoder can also be trained to remove noise from images. We can add noise to inputs and train to recover the original noise-free inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWqaa8yc2ire"
      },
      "source": [
        "## Clear Model and Generate Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAzFQUZPUiMF"
      },
      "source": [
        "Clear previous model sessions and generate a seed for reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uzM_ovf2ix4"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOGGlaVKuTCV"
      },
      "source": [
        "## Build the Encoder with Gaussian Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB3f0LzfUkWa"
      },
      "source": [
        "Add pure Gaussian noise directly in the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QUF0DKhtQfr"
      },
      "source": [
        "from tensorflow.keras.layers import GaussianNoise\n",
        "\n",
        "gaussian_encoder = Sequential([\n",
        "  Flatten(input_shape=in_shape),\n",
        "  GaussianNoise(0.2),\n",
        "  dense_1,\n",
        "  dense_2,\n",
        "  dense_3\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdjwbm_-vPYK"
      },
      "source": [
        "## Build the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpwe3K5BUrLl"
      },
      "source": [
        "Tie the weights of the decoder layers to the weights of the encoder layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuCsUfsOvS__"
      },
      "source": [
        "gaussian_decoder = Sequential([\n",
        "  DenseTranspose(dense_3, activation='relu'),\n",
        "  DenseTranspose(dense_2, activation='relu'),\n",
        "  DenseTranspose(dense_1, activation='sigmoid'),\n",
        "  Reshape([28, 28])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIve4G3rvjla"
      },
      "source": [
        "## Build the Denoising Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIzpTj_sUvNy"
      },
      "source": [
        "Build the denoising autoencoder from the gaussian encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2DmmOHvvjsl"
      },
      "source": [
        "gaussian_ae = Sequential([gaussian_encoder, gaussian_decoder])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcqdzgAyvjw5"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrb75ynCU1-7"
      },
      "source": [
        "Compile with **binary crossentropy**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9UkDRUQwJlm"
      },
      "source": [
        "gaussian_ae.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=opt, metrics=[rounded_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr1VBHJkwNM8"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N46vycU7U7eK"
      },
      "source": [
        "Train model for ten epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGRxQqPzvj1g"
      },
      "source": [
        "gae_history = gaussian_ae.fit(\n",
        "    x_train, x_train, epochs=10,\n",
        "    validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWyIM-xLuoYE"
      },
      "source": [
        "Visualize training performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9vrqyWouodU"
      },
      "source": [
        "viz_history(tied_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4-zY0kevj5Z"
      },
      "source": [
        "## Visualize Reconstructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWcSldeyVA9C"
      },
      "source": [
        "Add the same amount of Gaussian noise to **test** images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rPYMlMovj9r"
      },
      "source": [
        "noise = GaussianNoise(0.2)\n",
        "show_reconstructions(gaussian_ae, noise(x_test_imgs), 6)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWuNdF9CxYHt"
      },
      "source": [
        "# Build the Encoder with Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IothDH0fXps2"
      },
      "source": [
        "Add dropout directly into the encoder. Dropout adds random noise to the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OkuBpiaxYOY"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "dropout_encoder = Sequential([\n",
        "  Flatten(input_shape=in_shape),\n",
        "  Dropout(0.5),\n",
        "  dense_1,\n",
        "  dense_2,\n",
        "  dense_3\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7gBrIomtQj0"
      },
      "source": [
        "## Build the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBmsk4BYXzO2"
      },
      "source": [
        "Tie the weights of the decoder layers to the weights of the encoder layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dtoaqDdtQoi"
      },
      "source": [
        "dropout_decoder = Sequential([\n",
        "  DenseTranspose(dense_3, activation='relu'),\n",
        "  DenseTranspose(dense_2, activation='relu'),\n",
        "  DenseTranspose(dense_1, activation='sigmoid'),\n",
        "  Reshape([28, 28])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6daiOrUX13t"
      },
      "source": [
        "We tie the weights together because the performance is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NmAnKSr1mPp"
      },
      "source": [
        "## Build the Dropout Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wMXDBzcYH_l"
      },
      "source": [
        "Build the autoencoder from the dropout encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn-JY_mT1mU9"
      },
      "source": [
        "dropout_ae = Sequential([dropout_encoder, dropout_decoder])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V92OPjsD10Y0"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NvOgcYbYNud"
      },
      "source": [
        "Compile with **binary crossentropy**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJwYCpjb10qZ"
      },
      "source": [
        "dropout_ae.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=opt, metrics=[rounded_accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CjRl2sj104z"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Q007HoYRU-"
      },
      "source": [
        "Train the model for ten epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mugmk_m109v"
      },
      "source": [
        "drop_history = dropout_ae.fit(\n",
        "    x_train, x_train, epochs=10,\n",
        "    validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxgz9a5x2Xv7"
      },
      "source": [
        "Visualize performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ewAbOQY2X3U"
      },
      "source": [
        "viz_history(drop_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHDqIei44GzR"
      },
      "source": [
        "## Visualize Reconstructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wje_Oz-MYWwj"
      },
      "source": [
        "Add the same amount of dropout noise to test images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afm97V2C4G38"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "dropout = Dropout(0.5)\n",
        "show_reconstructions(dropout_ae, dropout(x_test_imgs), 6)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}