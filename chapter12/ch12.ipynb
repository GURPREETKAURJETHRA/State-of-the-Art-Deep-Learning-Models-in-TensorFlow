{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch12.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E0DIZQALOE23"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0DIZQALOE23"
      },
      "source": [
        "# Fast Style Transfer for Arbitrary Styles\n",
        "\n",
        "Neural Style Transfer uses deep learning to compose one image in the style of another image. **Fast Style Transfer** is a faster and better implementation of Neural Style that combines the description of one image with the style of another image through convolutional neural networks.\n",
        "\n",
        "Resources:\n",
        "\n",
        "https://www.techleer.com/articles/466-insight-into-fast-style-transfer-in-tensorflow/\n",
        "\n",
        "https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization\n",
        "\n",
        "https://www.tensorflow.org/tutorials/generative/style_transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioHCOdEcTX1X"
      },
      "source": [
        "# Import **tensorflow** library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpygsU5NTY4O"
      },
      "source": [
        "Import library and alias it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prmzt6Q0TdMM"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1PaVreZTcPm"
      },
      "source": [
        "# GPU Hardware Accelerator\n",
        "\n",
        "To vastly speed up processing, we can use the GPU available from the Google Colab cloud service. Colab provides a free Tesla K80 GPU of about 12 GB. It’s very easy to enable the GPU in a Colab notebook:\n",
        "\n",
        "1.\tclick **Runtime** in the top left menu\n",
        "2.\tclick **Change runtime** type from the drop-down menu\n",
        "3.\tchoose **GPU** from the Hardware accelerator drop-down menu\n",
        "4.\tclick **SAVE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu9mIFhNTgeW"
      },
      "source": [
        "Verify that GPU is available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-LVWYafThG_"
      },
      "source": [
        "tf.__version__, tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2mmzhqjz-bv"
      },
      "source": [
        "# Fast Style Transfer Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcllIuuxVKfX"
      },
      "source": [
        "## Import Requisite Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5eRwZX4VN8e"
      },
      "source": [
        "Import libraries that we need for this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64LS09veVKkf"
      },
      "source": [
        "from matplotlib import gridspec\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4FHO1NvlCKj"
      },
      "source": [
        "## Get Images from Google Drive\n",
        "\n",
        "Be sure that the images are in the **Colab Notebooks** directory on Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh1MePTulE07"
      },
      "source": [
        "Mount Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQeavU4MWVyY"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdCpcIOPLbCS"
      },
      "source": [
        "Load and display the style image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKI_qrONju4q"
      },
      "source": [
        "img_path = 'gdrive/My Drive/Colab Notebooks/images/serene.jpeg'\n",
        "style = Image.open(img_path)\n",
        "plt.axis('off')\n",
        "_ = plt.imshow(style)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "savDRQsWNYOs"
      },
      "source": [
        "Get size of style image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSYL26b0NYV7"
      },
      "source": [
        "w, h = style.size\n",
        "w, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg5AF0FEOCiU"
      },
      "source": [
        "Get the image type:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq1TvFiLUa8o"
      },
      "source": [
        "type(style)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iSCRgV9UhFI"
      },
      "source": [
        "The image is a PIL image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuMKvBPOLq68"
      },
      "source": [
        "Load and display the content image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfNnKWFYjL3x"
      },
      "source": [
        "img_path = 'gdrive/My Drive/Colab Notebooks/images/'\\\n",
        "  'humming_bird.jpeg'\n",
        "content  = Image.open(img_path)\n",
        "plt.axis('off')\n",
        "_ = plt.imshow(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Z2jWikNzgX"
      },
      "source": [
        "Get size of content image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuThKHtUNzns"
      },
      "source": [
        "w, h = content.size\n",
        "w, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxk0exmDUo9I"
      },
      "source": [
        "Get the image type:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79YRyp4YUpEI"
      },
      "source": [
        "type(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1OvjXIHUxpX"
      },
      "source": [
        "Since both images are PIL images, we must convert them to tensors before we can feed them to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C5xcLZ6k9Tc"
      },
      "source": [
        "## Preprocess Images\n",
        "\n",
        "The recommended size for the style image is 256 x 256 because this is the size expected by the pre-trained style transfer network we use for this experiment. The content image can be any size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9XSpemkL9ic"
      },
      "source": [
        "Convert the style image to a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZC_yjRdLeKx"
      },
      "source": [
        "style_array = tf.keras.preprocessing.image.img_to_array(\n",
        "    style) / 255.\n",
        "style_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeUM8zPyTO-n"
      },
      "source": [
        "A PIL image is not TensorFlow consumable. So we convert it to a NumPy array and scale it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtv8Sp-OTFtQ"
      },
      "source": [
        "Resize the style tensor for the style transfer network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGqVrxrGTF04"
      },
      "source": [
        "style_img = tf.image.resize(style_array, (256, 256))\n",
        "style_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJbKhq7XUQlx"
      },
      "source": [
        "Voilà. Convert the content image to a NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HefYYdNMGUR"
      },
      "source": [
        "content_img = tf.keras.preprocessing.image.img_to_array(\n",
        "    content) / 255.\n",
        "content_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VY5I409WxML"
      },
      "source": [
        "We did not resize the content image because it can be any size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAJ6T8rwlG_r"
      },
      "source": [
        "## Display Processed Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwGBZbC5W-Qh"
      },
      "source": [
        "Create a function to display images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBWKFgZ1cdik"
      },
      "source": [
        "def display_one(img):\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsDHqwFGume8"
      },
      "source": [
        "Display the processed style image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRzfuloFch98"
      },
      "source": [
        "display_one(style_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHLuyepjupPl"
      },
      "source": [
        "Display the processed content image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29dsyb8Hjet5"
      },
      "source": [
        "display_one(content_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6YXIgnEhzvW"
      },
      "source": [
        "## Prepare Images for the Network\n",
        "\n",
        "Content and style images are expected to be 4D Tensors with shapes `[batch_size, image_height, image_width, 3]` by the pre-trained model we use in this experiment. Our content and style images are 3D tensors with shapes `[image_height, image_width, 3]`. So we must add a batch dimension of 1 to make them consumable by the model. We can use the same module to process more images at the same time, but batch size would equal the number of images in this case.\n",
        "\n",
        "Input and output values of the images are expected to be in the range [0, 1]. We scaled both images to meet this requirement. Shapes of content and style image don't have to match. Output image shape is adapted from the content image shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O714AAa0Cvb"
      },
      "source": [
        "Add the batch dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AFegifCzw8S"
      },
      "source": [
        "style_image = np.expand_dims(style_img, axis=0)\n",
        "content_image = np.expand_dims(content_img, axis=0)\n",
        "style_image.shape, content_image.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3acCq0awUpl"
      },
      "source": [
        "Convert numpy images to TensorFlow tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuyKPb6TsVdo"
      },
      "source": [
        "style_tensor = tf.convert_to_tensor(style_image)\n",
        "content_tensor = tf.convert_to_tensor(content_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hLompocb-jm"
      },
      "source": [
        "The style transfer network accepts TensorFlow tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsbUu5NSwcFs"
      },
      "source": [
        "## Load the Model\n",
        "\n",
        "We use the **arbitrary-image-stylization-v1-256** network. The network is a pre-trained model for fast arbitrary image style transfer. The network doesn't require that images be resized, but prefers that style images are about 256 pixels because it is trained on 256 x 256 pixel images. But content images can be any size.\n",
        "\n",
        "NST models are normally limited to a pre-selected handful of styles because a separate neural network must be trained for each style image. Arbitrary style transfer mitigates the limitation of basic NST models by using a style network and transformer network. The style network learns how to break down an image into a 100-dimensional vector (or style vector) that represents its style. The transformer network learns how to produce the final stylized image from the style vector and original content image.\n",
        "\n",
        "The original NST networks requires a slow iterative optimization process that limits its practical application. Fast approximations with feed-forward neural networks are faster, they are tied to a fixed set of styles. So they cannot adapt to arbitrary new styles. The latest fast style networks enable arbitrary style transfer in real-time. So they are referred to as fast style transfer for abritrary styles. At this new network is an adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. The new network achieves speed comparable to the fastest existing approaches without the restriction of a pre-defined set of styles. The approach allows flexible user controls such as content-style trade-off, style interpolation, and color & spatial controls using a single feedforward neural network.\n",
        "\n",
        "To view the documentation for the arbitrary network, peruse:\n",
        "\n",
        "https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFVBZ0K_wdWU"
      },
      "source": [
        "Load the pre-trained style transfer network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwwStJ_csK2i"
      },
      "source": [
        "p1 = 'https://tfhub.dev/google/magenta/'\n",
        "p2 = 'arbitrary-image-stylization-v1-256/2'\n",
        "URL = p1 + p2\n",
        "\n",
        "hub_handle = URL\n",
        "hub_module = hub.load(hub_handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHwIhoygkpzT"
      },
      "source": [
        "Build the hub module from pre-trained fast arbitrary image style transfer network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgTYMJYawkUE"
      },
      "source": [
        "## Demonstrate Image Stylization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4-MOuBlwr3U"
      },
      "source": [
        "Feed the model a content image and style reference image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHIc_LjvsOph"
      },
      "source": [
        "outputs = hub_module(content_tensor, style_tensor)\n",
        "pastiche = outputs[0]\n",
        "pastiche.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlVerejhkgUs"
      },
      "source": [
        "The signature of the hub module for image stylization accepts the processed content image and the processed style reference image to create a stylized image from learning how to blend the two tensors. A **pastiche** is an artistic work in a style that imitates that of another work, artist or period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y-RYPk8xpMv"
      },
      "source": [
        "Training is fast with the GPU! Training the hub module signature with a CPU takes some time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgnhp_2J0l2S"
      },
      "source": [
        "## Explore the Stylized Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca_SQFf4j-Fe"
      },
      "source": [
        "Explore the stylized NumPy image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN6rrYFMjFNo"
      },
      "source": [
        "pastiche_numpy = tf.squeeze(pastiche).numpy()\n",
        "pastiche_numpy.shape, content_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRKumjWHmTGs"
      },
      "source": [
        "Convert the new stylized image tensor to NumPy for easy exploration. The shape is not exactly the same as the content image, but very close."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRItyiFQm7AS"
      },
      "source": [
        "Explore a slice from the stylized image tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSHtCaremTS1"
      },
      "source": [
        "pastiche_numpy[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-pntUQhbw0c"
      },
      "source": [
        "As expected, it has the same pixel characteristics as any other image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5XFM_AU09fM"
      },
      "source": [
        "Extract matrix components:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWOl7I_G09qL"
      },
      "source": [
        "m = pastiche_numpy\n",
        "r, c, channels = m.shape[0], m.shape[1], m.shape[2]\n",
        "r, c, channels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eexbRNvx582c"
      },
      "source": [
        "The output image is a 3D matrix consisting of 184 rows, 280 columns, and 3 channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcPDFF6f2szx"
      },
      "source": [
        "Get number of pixels in the matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORBkvmNt2s84"
      },
      "source": [
        "pixels = r * c\n",
        "pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Hd1PLn3V-K"
      },
      "source": [
        "So each RGB channel has 51,520 pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq0jyUFhrjj1"
      },
      "source": [
        "Check if RGB channel pixels are scaled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPZNgdX_rjqQ"
      },
      "source": [
        "red = m[m[:, :, 0] < 1, 0] < 1\n",
        "green = m[m[:, :, 1] < 1, 0] < 1\n",
        "blue = m[m[:, :, 2] < 1, 0] < 1\n",
        "print (len(red), len(green), len(blue))\n",
        "print (red, green, blue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2kt09u76cTT"
      },
      "source": [
        "The algorithms check if the pixels in each channel are less than one. Each channel has the expected number of pixels. Since all truth table values aren't displayed, we need one more step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAdZPZyU3Sg0"
      },
      "source": [
        "all(red), all(green), all(blue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzCMOUdm3pON"
      },
      "source": [
        "The all() function returns True if all items in an iterable are true, otherwise it returns False. So all pixels are scaled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M5OhAHl7bl-"
      },
      "source": [
        "We can check if pixels are scaled in one step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6qBDSVS47Ah"
      },
      "source": [
        "truth = np.where((m < 1), True, False)\n",
        "truth.all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HafybCLB7k92"
      },
      "source": [
        "We showed the multistep process to provide a glimpse under the hood of the stylized image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JemkqyIRw8iM"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4jEiOQew4DE"
      },
      "source": [
        "Create a visualization function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90HxmtwTswIZ"
      },
      "source": [
        "def show_n(images, titles=('',)):\n",
        "  n = len(images)\n",
        "  image_sizes = [image.shape[1] for image in images]\n",
        "  w = (image_sizes[0] * 6) // 320\n",
        "  plt.figure(figsize=(w  * n, w))\n",
        "  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)\n",
        "  for i in range(n):\n",
        "    plt.subplot(gs[i])\n",
        "    plt.imshow(images[i][0], aspect='equal')\n",
        "    plt.axis('off')\n",
        "    plt.title(titles[i] if len(titles) > i else '')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIf5aXaIw9rc"
      },
      "source": [
        "Visualize original content image, style image, and stylized image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAU97ckUswrB"
      },
      "source": [
        "show_n([content_image, style_image, pastiche],\n",
        "       titles=['Original content image', 'Style image',\n",
        "               'Pastiche'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpdEZjCF9Sfa"
      },
      "source": [
        "Create a function to display the new image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQsr6HyH9Soq"
      },
      "source": [
        "def display_pastiche(img, size):\n",
        "  plt.figure(figsize = size)\n",
        "  plt.imshow(tf.squeeze(img))\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGmkY66w760H"
      },
      "source": [
        "Display the stylized image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzd1owAl8df4"
      },
      "source": [
        "f_size = (10, 15)\n",
        "display_pastiche(pastiche, f_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIAdFX56TSKd"
      },
      "source": [
        "## Image Stylization with Multiple Images\n",
        "\n",
        "Create lists of style reference and content images. Choose one from each list to create a pastiche."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WulqoSR0i2Yl"
      },
      "source": [
        "### Get Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bBqUqLsbnZi"
      },
      "source": [
        "Create a function to grab an image from Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvO2mdfLThXM"
      },
      "source": [
        "def get_image(img_path):\n",
        "  return Image.open(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv6WntUMTSaz"
      },
      "source": [
        "Grab style images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA5T7uuzTSho"
      },
      "source": [
        "d = 'gdrive/My Drive/Colab Notebooks/images/dali.jpg'\n",
        "dali = get_image(d)\n",
        "v = 'gdrive/My Drive/Colab Notebooks/images/van-gogh.jpg'\n",
        "van_gogh = get_image(v)\n",
        "m = 'gdrive/My Drive/Colab Notebooks/images/modern.jpg'\n",
        "modern = get_image(m)\n",
        "e = 'gdrive/My Drive/Colab Notebooks/images/escher.jpeg'\n",
        "escher = get_image(e)\n",
        "pic = 'gdrive/My Drive/Colab Notebooks/images/picasso.jpg'\n",
        "picasso = get_image(pic)\n",
        "p = 'gdrive/My Drive/Colab Notebooks/images/pollock.jpg'\n",
        "pollock = get_image(p)\n",
        "mon = 'gdrive/My Drive/Colab Notebooks/images/monet.jpg'\n",
        "monet = get_image(mon)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouOcu80siVQz"
      },
      "source": [
        "Display style reference images sizes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orKAEeQhiVYp"
      },
      "source": [
        "dali.size, van_gogh.size, modern.size, escher.size,\\\n",
        "picasso.size, pollock.size, monet.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTdHI9pbcxnH"
      },
      "source": [
        "Grab content images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA9KrnRjcx5f"
      },
      "source": [
        "t = 'gdrive/My Drive/Colab Notebooks/images/teddy.jpeg'\n",
        "teddy = get_image(t)\n",
        "e = 'gdrive/My Drive/Colab Notebooks/images/einstein.jpg'\n",
        "einstein = get_image(e)\n",
        "g = 'gdrive/My Drive/Colab Notebooks/images/gem.jpeg'\n",
        "gem = get_image(g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ntCg7mihKp"
      },
      "source": [
        "Display original content image sizes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j3R5rihihVL"
      },
      "source": [
        "teddy.size, einstein.size, gem.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9mNA4wTdR-o"
      },
      "source": [
        "### Process Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bP1Rk4LTSqb"
      },
      "source": [
        "Create a preprocessing function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j64mQcmufyD1"
      },
      "source": [
        "def preprocess(img, style=True):\n",
        "  img_array = tf.keras.preprocessing.image.img_to_array(\n",
        "      img) / 255.\n",
        "  if style:\n",
        "    img_array = tf.image.resize(img_array, (256, 256))\n",
        "  return\\\n",
        "  tf.convert_to_tensor(np.expand_dims(img_array, axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0sZ_zqsgsJu"
      },
      "source": [
        "Process the style images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X_1Sz9yegmk"
      },
      "source": [
        "dali_style = preprocess(dali)\n",
        "van_gogh_style = preprocess(van_gogh)\n",
        "modern_style = preprocess(modern)\n",
        "escher_style = preprocess(escher)\n",
        "picasso_style = preprocess(picasso)\n",
        "pollock_style = preprocess(pollock)\n",
        "monet_style = preprocess(monet)\n",
        "\n",
        "dali_style.shape, van_gogh_style.shape, modern_style.shape,\\\n",
        "escher_style.shape, picasso_style.shape, pollock_style.shape,\\\n",
        "monet_style.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI9pBhMsgvsT"
      },
      "source": [
        "Process the content images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESXXqQkhfier"
      },
      "source": [
        "einstein_content = preprocess(einstein, False)\n",
        "teddy_content = preprocess(teddy, False)\n",
        "gem_content = preprocess(gem, False)\n",
        "einstein_content.shape, teddy_content.shape, gem_content.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zCl6rt_jBz0"
      },
      "source": [
        "### Visualize Processed Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVUmdpdxjyUO"
      },
      "source": [
        "Place images in lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pintJLdZj2_S"
      },
      "source": [
        "styles = [dali_style, van_gogh_style, modern_style,\n",
        "          escher_style, picasso_style, pollock_style,\n",
        "          monet_style]\n",
        "contents = [einstein_content, teddy_content, gem_content]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cokZN55ykEpn"
      },
      "source": [
        "Create a function to display tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRHJsPwXkFG-"
      },
      "source": [
        "def display_tensors(imgs, r, c):\n",
        "  _, axs = plt.subplots(r, c, figsize=(12, 12))\n",
        "  axs = axs.flatten()\n",
        "  for img, ax in zip(imgs, axs):\n",
        "    ax.imshow(tf.squeeze(img))\n",
        "    ax.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uruae5OlZBA"
      },
      "source": [
        "Display style tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_dfVt1jlPua"
      },
      "source": [
        "rows, cols = 1, 7\n",
        "display_tensors(styles, rows, cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10mgg-XXlb94"
      },
      "source": [
        "Display content tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q8m8VV_lfQs"
      },
      "source": [
        "rows, cols = 1, 3\n",
        "display_tensors(contents, rows, cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsIPVG6mpR4u"
      },
      "source": [
        "### Create Reference Dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bucJrC4Hoj20"
      },
      "source": [
        "Create a dictionary to represent style tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GClrgT9WnExc"
      },
      "source": [
        "style_names = {'dali' : styles[0],\n",
        "               'van_gogh' : styles[1],\n",
        "               'modern' : styles[2],\n",
        "               'escher' : styles[3],\n",
        "               'picasso' : styles[4],\n",
        "               'pollock' : styles[5],\n",
        "               'monet' : styles[6]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRQI92LqousV"
      },
      "source": [
        "Create a dictionary to represent content tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXivl1nuou10"
      },
      "source": [
        "content_names = {'einstein' : contents[0],\n",
        "                 'teddy' : contents[1],\n",
        "                 'gem' : contents[2]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZNlTccKmEfi"
      },
      "source": [
        "### Create Stylized Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yimz5UVWy_dm"
      },
      "source": [
        "Create a function that creates a pastiche:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qMMM97ey_kF"
      },
      "source": [
        "def create(c, s):\n",
        "  content_im = content_names[c]\n",
        "  style_im = style_names[s]\n",
        "  outputs = hub_module(content_im, style_im)\n",
        "  return content_im, style_im, outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-DOW-1xzt0w"
      },
      "source": [
        "Create and display a pastiche:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjcWqtdzzTr1"
      },
      "source": [
        "content, style = 'einstein', 'dali'\n",
        "content_im, style_im, sim = create(content, style)\n",
        "\n",
        "f_size = (7, 9)\n",
        "display_pastiche(sim, f_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4eSAAJHz1eX"
      },
      "source": [
        "Feed the function a content and style reference image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh7zpeNJrTm2"
      },
      "source": [
        "Place images in a list for visualization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtAfZQgxsaJZ"
      },
      "source": [
        "imgs = [content_im, style_im, sim]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPf4mdYusaRX"
      },
      "source": [
        "Display the original content and style reference images along with the pastiche."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFN6qI06rTwe"
      },
      "source": [
        "display_tensors(imgs, 1, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLaTJOiuyaFq"
      },
      "source": [
        "Try one with Teddy Roosevelt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKl9qu-8UOhk"
      },
      "source": [
        "content, style = 'teddy', 'picasso'\n",
        "content_im, style_im, sim = create(content, style)\n",
        "\n",
        "f_size = (8, 10)\n",
        "display_pastiche(sim, f_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ55a3McUvTG"
      },
      "source": [
        "Display tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOiZCCI-Uvap"
      },
      "source": [
        "imgs = [content_im, style_im, sim]\n",
        "display_tensors(imgs, 1, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvzPs4VOUOqM"
      },
      "source": [
        "Try one with the gem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI_vOxA8yaP5"
      },
      "source": [
        "content, style = 'gem', 'escher'\n",
        "content_im, style_im, sim = create(content, style)\n",
        "\n",
        "f_size = (8, 10)\n",
        "display_pastiche(sim, f_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yBov-5Aycd6"
      },
      "source": [
        "Display tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4piE7kPxycmz"
      },
      "source": [
        "imgs = [content_im, style_im, sim]\n",
        "display_tensors(imgs, 1, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAG7d_vO3xG1"
      },
      "source": [
        "# Stylize Images with TensorFlow Lite\n",
        "\n",
        "**TensorFlow Lite** is an open-source deep learning framework to run TensorFlow models on-device. On-Device Portals (ODPs) allow mobile phone users to easily browse, purchase, and use mobile content and services. An ODP platform enables operators to provide a consistent and branded on-device experience across a large portfolio of services. TensorFlow Lite provides an ODP platform to experiment with and deploy deep learning experiments on their phone.\n",
        "\n",
        "To get started with TensorFlow Lite on your device, peruse:\n",
        "\n",
        "https://www.tensorflow.org/lite/examples\n",
        "\n",
        "We run the experiment in a Colab notebook on a PC. The reason is that on-device RAM is not equal to what we have on our PC. So TensorFlow Lite was developed with on-device limitations (and benefits) in mind.\n",
        "\n",
        "For an excellent site for on-device TensorFlow Lite, peruse:\n",
        "\n",
        "https://www.tensorflow.org/lite/examples/style_transfer/overview\n",
        "\n",
        "TensorFlow Lite is a serious on-device product. So you have to install the appropriate app for your specific device."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o94JjqUK8O9l"
      },
      "source": [
        "## Architecture for a Pre-Trained TensorFlow Lite Model\n",
        "\n",
        "Content images are exactly the same as what we have already worked with in the first experiment. Style images begin the same but are transformed (or bottlenecked) into `100-dimensional` style bottleneck vectors before being fed into the style transform model.\n",
        "\n",
        "The atistic style transfer model consists of two submodels - Style Prediciton Model and Style Transform Model. The **Style Prediction Model** is a pre-trained MobilenetV2-based neural network that takes an input style image and transformes it into a `100-dimensional` style bottleneck vector. The **Style Transform Model** is a neural network that applies a style bottleneck vector to a content image to create a pastiche."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsiVQkhhtJJv"
      },
      "source": [
        "## Prepare Images\n",
        "\n",
        "Images are already processed. But we must prepare the style image for the Style Prediction Model and the content image for the Style Transform model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKdISQYunbV5"
      },
      "source": [
        "Import a requisite library for display:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FyQYE0DM8sD"
      },
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJHHFuhx-JGi"
      },
      "source": [
        "The mpl parameters set the display size for all images for the rest of the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WchSNryN_6-T"
      },
      "source": [
        "Get a content image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-6TprQW_7E_"
      },
      "source": [
        "content_cd = content_names['einstein']\n",
        "content_cd.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qccs9BPGmhQU"
      },
      "source": [
        "The Style Transform Model expects a content image of size 384 x 384: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4cg9h8FmhYy"
      },
      "source": [
        "dim = [384, 384]\n",
        "content_lte = tf.image.resize(content_cd, dim)\n",
        "content_lte.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqHwH0RTV6l_"
      },
      "source": [
        "Centrally crop the content image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR9nr6alV6vF"
      },
      "source": [
        "content_lite = tf.image.resize_with_crop_or_pad(\n",
        "    content_lte, dim[0], dim[1])\n",
        "content_lite.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbIkcTbNXWaw"
      },
      "source": [
        "Cropping is one of the most basic data augmentation processes for images. The idea is to remove unwanted or irrelevant noise from the periphery of an image, change its aspect ratio, or improve its overall composition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkNBdbRZ_7MR"
      },
      "source": [
        "Get a style image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg4Xk8_l_7Rk"
      },
      "source": [
        "style_lte = style_names['modern']\n",
        "style_lte.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1KY8KQIWbgP"
      },
      "source": [
        "Centrally crop the style image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBf8IQDDWbrW"
      },
      "source": [
        "dim = [256, 256]\n",
        "style_lite = tf.image.resize_with_crop_or_pad(\n",
        "    style_lte, dim[0], dim[1])\n",
        "style_lite.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v34d00SntVrX"
      },
      "source": [
        "## Display Prepared Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyRf51J0_7Yf"
      },
      "source": [
        "Create a display function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_3OcRul_7fS"
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "  plt.axis('off')    \n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4duxQQTWClq1"
      },
      "source": [
        "Display content and style images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv9wamFQClwU"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_lite, 'Content Image')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_lite, 'Style Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKpupkEuF3z"
      },
      "source": [
        "## Create the Style Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E01_IJVh_7m9"
      },
      "source": [
        "Create a function for the Style Prediction Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypNHBhPHC4C0"
      },
      "source": [
        "def run_style_predict(processed_style_image):\n",
        "  # load the model\n",
        "  interpreter = tf.lite.Interpreter(\n",
        "      model_path=style_predict_path)\n",
        "  # set model input\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()\n",
        "  interpreter.set_tensor(\n",
        "      input_details[0][\"index\"], processed_style_image)\n",
        "  # calculate style bottleneck\n",
        "  interpreter.invoke()\n",
        "  style_bottleneck = interpreter.tensor(\n",
        "      interpreter.get_output_details()[0][\"index\"])()\n",
        "  return style_bottleneck"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiSHKYfmDUot"
      },
      "source": [
        "The function creates a Style Prediction Model to run style prediction on a processed style image. The function begins by loading the style prediction path into the **interpreter** object (tf.lite.Interpreter), which is a TensorFlow Lite pre-trained MobilenetV2-based neural network. The interpreter uses the style_predict_path to create a tflite predicted style. Input to the interpreter model is then set. The function continues by calculating the style bottleneck. The function ends by using the bottleneck calculation and the tflite predicted style to transform the style image into a 100-dimensional style bottleneck vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCgMDZgCETsT"
      },
      "source": [
        "Set the style predict path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w07JE0LMHnDR"
      },
      "source": [
        "tflite_predict = 'style_predict.tflite'\n",
        "p1 = 'https://tfhub.dev/google/lite-model/magenta/'\n",
        "p2 = 'arbitrary-image-stylization-v1-256/'\n",
        "p3 = 'int8/prediction/1?lite-format=tflite'\n",
        "URL = p1 + p2 + p3\n",
        "\n",
        "style_predict_path = tf.keras.utils.get_file(\n",
        "    tflite_predict, URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWpTz8PXHqZv"
      },
      "source": [
        "Transform the style image into a 100-dimensional style bottleneck vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKTnGL-tHqgj"
      },
      "source": [
        "style_bottleneck = run_style_predict(style_lite)\n",
        "print('style bottleneck vector shape:',\n",
        "      style_bottleneck.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciIwQSZduLR_"
      },
      "source": [
        "## Create the Style Transform Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uWwPomBHqon"
      },
      "source": [
        "Create a function for the Style Transform Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiKQ1AeSHqvp"
      },
      "source": [
        "def run_style_transform(\n",
        "    style_bottleneck, processed_content_image):\n",
        "  # load the model\n",
        "  interpreter = tf.lite.Interpreter(\n",
        "      model_path=style_transform_path)\n",
        "  # set model input\n",
        "  input_details = interpreter.get_input_details()\n",
        "  interpreter.allocate_tensors()\n",
        "  # set content and style bottleneck\n",
        "  interpreter.set_tensor(\n",
        "      input_details[0][\"index\"], processed_content_image)\n",
        "  interpreter.set_tensor(\n",
        "      input_details[1][\"index\"], style_bottleneck)\n",
        "  interpreter.invoke()\n",
        "  # return the transformed content image\n",
        "  return interpreter.tensor(\n",
        "      interpreter.get_output_details()[0][\"index\"])()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMyDuBQ9LQMz"
      },
      "source": [
        "The function creates a Style Transform Model that applies a style bottleneck vector to a content image to create a pastiche. The function begins by loading the style transform path into the **interpreter** object (tf.lite.Interpreter). The interpreter uses the style_transform_path to create a tflite transform style. Input to the interpreter model is then set. The function continues by setting the processed content image and style bottleneck for the interpreter. The function ends by returning the transformed content image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvwSGL3MCecE"
      },
      "source": [
        "Set the style transform path:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIQMiIt-LQby"
      },
      "source": [
        "tflite_transform= 'style_transform.tflite'\n",
        "p1 = 'https://tfhub.dev/google/lite-model/magenta/'\n",
        "p2 = 'arbitrary-image-stylization-v1-256/'\n",
        "p3 = 'int8/transfer/1?lite-format=tflite'\n",
        "URL = p1 + p2 + p3\n",
        "\n",
        "style_transform_path = tf.keras.utils.get_file(\n",
        "    tflite_transform, URL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkmZzKYGuSxd"
      },
      "source": [
        "## Create the Pastiche"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwh4vv5eLQuZ"
      },
      "source": [
        "Stylize the content image using the style bottleneck:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3YZ1M0_LQ3R"
      },
      "source": [
        "stylized_image = run_style_transform(\n",
        "    style_bottleneck, content_lite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aikPqEVJD68g"
      },
      "source": [
        "Resize the stylized image to the size expected by the TensorFlow Lite pre-trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC8JPe0xD7D_"
      },
      "source": [
        "pastiche = tf.image.resize(stylized_image, [384, 384])\n",
        "pastiche.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si3Q2rb0KlfA"
      },
      "source": [
        "Visualize the pastiche:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE2HtPLjscrF"
      },
      "source": [
        "imshow(pastiche, 'Pastiche')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3FoVjsGHq3T"
      },
      "source": [
        "## Style Blending\n",
        "\n",
        "By blending the style of the content image into the stylized output, we make the pastiche look more like the content image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8bsnfu9znr4"
      },
      "source": [
        "### Prepare the Content Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btcIm6MYvdfq"
      },
      "source": [
        "Reshape the content image for consumption by the Style Prediction Model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2_hPy2RvXWY"
      },
      "source": [
        "dim = [256, 256]\n",
        "content_blend = tf.image.resize_with_crop_or_pad(\n",
        "    content_lite, dim[0], dim[1])\n",
        "content_blend.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugtbfi1Pu1OW"
      },
      "source": [
        "Transform the reshaped content image into a 100-dimensional style bottleneck vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvs6hgtUM9JJ"
      },
      "source": [
        "style_bottleneck_content = run_style_predict(\n",
        "    content_blend)\n",
        "style_bottleneck_content.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYja4h7Lz0sP"
      },
      "source": [
        "### Blend the Style Bottleneck Vectors\n",
        "\n",
        "Blend the style bottleneck vector with the content bottleneck vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgdHjO_03xUr"
      },
      "source": [
        "Define the content blending ratio between 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFqrBYYzw5x2"
      },
      "source": [
        "content_blending_ratio = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss0lm8w5FFuK"
      },
      "source": [
        "The range of blending the content image to the pastiche is from 0% to 100%. To extract no style from the content image, assign 0%. To extract all of the style from the content image, assign 100%. Ideally, assign a reasonable percent of style extraction from the content image to get a reasonably blended image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ObgKQKxmAR"
      },
      "source": [
        "Create a blended style bottleneck vector from the style bottlenec and content-style bottleneck vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "malGXJxWxxyc"
      },
      "source": [
        "style_bottleneck_blended =\\\n",
        "  content_blending_ratio * style_bottleneck_content +\\\n",
        "  (1 - content_blending_ratio) * style_bottleneck"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5v2w75WyGri"
      },
      "source": [
        "Stylize the content image using the style bottleneck:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obXYRS1fyHU6"
      },
      "source": [
        "stylized_image_blended = run_style_transform(\n",
        "    style_bottleneck_blended, content_lite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4QNxtKAyVDx"
      },
      "source": [
        "Visualize the pastiche:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF2xpkzn3xcj"
      },
      "source": [
        "imshow(stylized_image_blended, 'Blended Stylized Image')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Bp7ezh1dHm"
      },
      "source": [
        "### Save the Pastiche\n",
        "\n",
        "Save the pastiche to a local drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMRzVQpi-c_g"
      },
      "source": [
        "Create a function to convert a tensor to a PIL image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWne_zAD7HcC"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  tensor = tensor * 255\n",
        "  tensor = np.array(tensor, dtype=np.uint8)\n",
        "  if np.ndim(tensor) > 3:\n",
        "    assert tensor.shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return Image.fromarray(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYpCkDgs-kzq"
      },
      "source": [
        "The function up-scales the scaled tensor and converts it to a NumPy array. It then strips the '1' dimension and returns a PIL image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RABAhsTZ_BEV"
      },
      "source": [
        "Save a file to a local drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doQGvnH56buP"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "fn = 'patiche.jpg'\n",
        "tensor_to_image(stylized_image_blended).save(fn)\n",
        "files.download(fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-5YG5mX_PPx"
      },
      "source": [
        "Import the files module from the google.colab library. Invoke the function and save method on the PIL image. Use the download method from the files module to download the PIL image to a local drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtnTkd2-_p5K"
      },
      "source": [
        "For additional Google Drive functionality, peruse:\n",
        "\n",
        "https://colab.research.google.com/notebooks/io.ipynb"
      ]
    }
  ]
}