{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLlkjVEbh7Kr"
      },
      "source": [
        "# Generative Adverserial Networks\n",
        "\n",
        "Generative Adverserial Networks (GANs) are generative models in that they create new data instances that resemble our training data. Generative modeling is an **unsupervised** learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.\n",
        "\n",
        "GANs generate a high level of realism in the new data instances that they create by pairing a generator that learns to produce the target output with a discriminator that learns to distinguish true data from the output of the generator. The generator tries to fool the discriminator and the discriminator tries to keep from being fooled.\n",
        "\n",
        "A **generator** is used to generate new plausible examples from the problem domain. It takes a random distribution as input (typically Gaussian) and outputs some data (typically an image). The random inputs can be thought of as the latent representations (or codings) of the image to be generated. So, generators have the same functionality as decoders in a variational autoencoder and can be used in the same way to generate new images. However, they are trained very differently.    \n",
        "\n",
        "A **discriminator** is used to classify examples as real (from the domain) or fake (generated). It takes either a fake image from the generator or a real image from the training set as input and guesses whether the input image is fake or real.\n",
        "\n",
        "During training, the generator and discriminator have opposite goals. The discriminator tries to distinguish fake images from real ones. The generator tries to produce images that look real enough to trick the discriminator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-K8N-MITUAH"
      },
      "source": [
        "# Import **tensorflow** library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDNRJCaseWUz"
      },
      "source": [
        "Import the library and alias it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ditCWH-lTUMm"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km-1djqqyZNN"
      },
      "source": [
        "# GPU Hardware Accelerator\n",
        "\n",
        "To vastly speed up processing, we can use the GPU available from the Google Colab cloud service. Colab provides a free Tesla K80 GPU of about 12 GB. Itâ€™s very easy to enable the GPU in a Colab notebook:\n",
        "\n",
        "1.\tclick **Runtime** in the top left menu\n",
        "2.\tclick **Change runtime** type from the drop-down menu\n",
        "3.\tchoose **GPU** from the Hardware accelerator drop-down menu\n",
        "4.\tclick **SAVE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWizKF0roTIS"
      },
      "source": [
        "Verify that GPU is active:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oqOsUQboQ4M"
      },
      "source": [
        "tf.__version__, tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP6rMB1-rk_Q"
      },
      "source": [
        "# Build a GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGPPkdcxpv1L"
      },
      "source": [
        "Load Fashion-MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00lYa0koW1F1"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "x_train_img, _ = tfds.as_numpy(\n",
        "    tfds.load('fashion_mnist', split='train',\n",
        "              batch_size=-1, as_supervised=True,\n",
        "              try_gcs=True, shuffle_files=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X28uZbFd_u9O"
      },
      "source": [
        "Since we only load train data, it is fine to shuffle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALZyTnVdXDTc"
      },
      "source": [
        "Number of samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaX2PJLuXDZL"
      },
      "source": [
        "len(x_train_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cRORQ_4qiQ1"
      },
      "source": [
        "Scale:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AypzKGvT1fWI"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "images = x_train_img.astype(np.float32) / 255\n",
        "images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc1_5BcrXM4c"
      },
      "source": [
        "Verify scaling worked as expected:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG08kabGXM9l"
      },
      "source": [
        "x_train_img[0][0], images[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXjDXN-orT_g"
      },
      "source": [
        "Get input shape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtayS1DCrUGZ"
      },
      "source": [
        "in_shape = images.shape[1:]\n",
        "in_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLbKg64f6epN"
      },
      "source": [
        "## Clear Previous Models and Generate Seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN-0Tyixezcw"
      },
      "source": [
        "Clear models and generate seed for reproducibility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1to24cIom7O"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B10uN1varO7N"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o2CSCW5e45H"
      },
      "source": [
        "Import requisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kcarZPrPAT"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten,\\\n",
        "  Reshape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zw0agus5j9i"
      },
      "source": [
        "## Build the Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp5Wz3tge9a4"
      },
      "source": [
        "The generator is similar to an autoencoder's decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfCrv7JZ5UBZ"
      },
      "source": [
        "codings_size = 30\n",
        "\n",
        "generator = Sequential([\n",
        "  Dense(32, activation='selu', input_shape=[codings_size]),\n",
        "  Dense(64, activation='selu'),\n",
        "  Dense(128, activation='selu'),\n",
        "  Dense(28 * 28, activation='sigmoid'),\n",
        "  Reshape(in_shape)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JukJ95wIreCA"
      },
      "source": [
        "## Build the Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRDoE2R7fAu3"
      },
      "source": [
        "The discriminator is a regular binary classifier. It takes an image as input and ends with a Dense layer containing a single unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET9Obu8SreH3"
      },
      "source": [
        "discriminator = Sequential([\n",
        "  Flatten(input_shape=in_shape),\n",
        "  Dense(128, activation='selu'),\n",
        "  Dense(64, activation='selu'),\n",
        "  Dense(32, activation='selu'),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjU574hu6r09"
      },
      "source": [
        "## Create the GAN Model\n",
        "\n",
        "Each training iteration is divided into two phases:\n",
        "\n",
        "* The first phase trains the discriminator. A batch of real images is sampled from the training set and an equal number of fake images is produced by the generator. Labels are set to 0 for fake images and 1 for real images. The discriminator is trained on the labeled batch for one step using binary cross entropy loss. Importantly, backpropagation only optimizes the weights of the discriminator during this phase.\n",
        "* The second phase trains the generator. We use the generator to produce another batch of fake images and the discriminator is used to distinguish between fake and real images. We don't add real images to the batch and all labels are set to 1 for real. We want the generator to produce images that the discriminator will incorrectly believe to be real! Crucially, weights of the discriminator are frozen during this step so backpropagation only affects weights of the generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJfoCpY2vMdj"
      },
      "source": [
        "Create model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DymJTsYw5UdZ"
      },
      "source": [
        "gan = Sequential([generator, discriminator])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLr05ZXSe2XW"
      },
      "source": [
        "## Compile the Discriminator Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khBu33LXfFuj"
      },
      "source": [
        "Since the discriminator is naturally a binary classifier (fake or real images), we naturally use binary cross-entropy loss. Since the generator is only trained through the gan model, we don't need to compile it. The gan model is also a binary classifier so it can use the same loss function. Importantly, the discriminator should not be trained during the second phase, so we make it non-trainable before compiling the gan model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i7x_C5se2ei"
      },
      "source": [
        "discriminator.compile(\n",
        "    loss='binary_crossentropy', optimizer='rmsprop')\n",
        "discriminator.trainable = False\n",
        "gan.compile(loss='binary_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXrhzsO5YZB3"
      },
      "source": [
        "## Build the Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJPlRKfDfIGo"
      },
      "source": [
        "Build the pipeline with batch size of 32:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2qLjjEFYZIA"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    images).shuffle(1000)\n",
        "dataset = dataset.batch(\n",
        "    batch_size, drop_remainder=True).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmyEfKqFwKVh"
      },
      "source": [
        "## Create a Custom Loop for Training\n",
        "\n",
        "Since the training loop is unusual, we can't use the regular fit method. Instead, we create a custom loop that needs a Dataset to iterate through images.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEfd23SCws4i"
      },
      "source": [
        "Create a function for the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na4qMYyWws9D"
      },
      "source": [
        "def train_gan(gan, dataset, batch_size,\n",
        "              codings_size, n_epochs=50):\n",
        "  generator, discriminator = gan.layers\n",
        "  for epoch in range(n_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
        "    for X_batch in dataset:\n",
        "      # phase 1 - training the discriminator\n",
        "      noise = tf.random.normal(\n",
        "          shape=[batch_size, codings_size])\n",
        "      generated_images = generator(noise)\n",
        "      X_fake_and_real = tf.concat(\n",
        "          [generated_images, X_batch], axis=0)\n",
        "      y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
        "      discriminator.trainable = True\n",
        "      discriminator.train_on_batch(X_fake_and_real, y1)\n",
        "      # phase 2 - training the generator\n",
        "      noise = tf.random.normal(\n",
        "          shape=[batch_size, codings_size])\n",
        "      y2 = tf.constant([[1.]] * batch_size)\n",
        "      discriminator.trainable = False\n",
        "      gan.train_on_batch(noise, y2)\n",
        "    plot_multiple_images(generated_images, 8)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15htkiHaxKeK"
      },
      "source": [
        "Phase one feeds Gaussian noise to the generator to produce fake images, target **y1** is set to 0 for fake and 1 for real images. The discriminator is then trained on the batch. Phase two feeds the GAN some Gaussian noise. Its generator starts by producing fake images. The discriminator then tries to guess if images are fake or real. We want the discriminaor to believe that the fake images are real, so targets **y2** are set to 1.\n",
        "\n",
        "Humans can easily ignore noise, but machine learning algorithms struggle. Small, human-imperceptible pixel changes, can dramatically alter a neural network's ability to make an accurate prediction. Research has shown that \n",
        "**noise** and **gaussian blurring** showed near immediate smoothing on tested models.\n",
        "\n",
        "Resource:\n",
        "\n",
        "https://blog.roboflow.com/why-to-add-noise-to-images-for-machine-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcU28Sv9zCNF"
      },
      "source": [
        "## Create a Function for Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxMZLx0-fPdw"
      },
      "source": [
        "Create a function to plot images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jE3wUv0zCTn"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_multiple_images(images, n_cols=None):\n",
        "  n_cols = n_cols or len(images)\n",
        "  n_rows = (len(images) - 1) // n_cols + 1\n",
        "  if images.shape[-1] == 1:\n",
        "    images = np.squeeze(images, axis=-1)\n",
        "  plt.figure(figsize=(n_cols, n_rows))\n",
        "  for index, image in enumerate(images):\n",
        "    plt.subplot(n_rows, n_cols, index + 1)\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmUP0J0GedEz"
      },
      "source": [
        "Generate images with the untrained generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GlSggLSJ-zF"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = generator(noise)\n",
        "plot_multiple_images(generated_images, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFmFRhDsKP3O"
      },
      "source": [
        "Not too impressive:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcldXo5byYPv"
      },
      "source": [
        "## Train the GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSRws1brfZJy"
      },
      "source": [
        "Train the GAN for a few epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgujmxYayYUE"
      },
      "source": [
        "n = 5\n",
        "\n",
        "train_gan(gan, dataset, batch_size, codings_size, n_epochs=n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syl4zzlk-hqH"
      },
      "source": [
        "Not great. We trained on 50 epochs. Images improved a little bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9joT3BgI64BW"
      },
      "source": [
        "## Generate Images with the Trained Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDoXmFFwfe7D"
      },
      "source": [
        "Add some Gaussian noise and use the generator model to generator some images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWnKZaJ35O-U"
      },
      "source": [
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = generator(noise)\n",
        "plot_multiple_images(generated_images, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCyPDKGo-BMO"
      },
      "source": [
        "There are 32 images generated because batch size is 32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2q6iOMn9jD3"
      },
      "source": [
        "Generating a seed ensures that generated images are the same. To generate different images each time, remove the seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW1-QFPt92DK"
      },
      "source": [
        "noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = generator(noise)\n",
        "plot_multiple_images(generated_images, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gGyiPsU69V_"
      },
      "source": [
        "<!-- Train for 10 epochs: -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_QGZnNx9KNS"
      },
      "source": [
        "# Deep Convolutional GANs (DCGANS)\n",
        "\n",
        "Using CNN architectures with GANS produces much better results than simple feedforward networks with GANs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-d9NqGDCDiu"
      },
      "source": [
        "## Create the Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoEcZWpXCHeh"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpbGHDNnCJVR"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization,\\\n",
        "  Conv2D, Conv2DTranspose, LeakyReLU, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USBOiEZ3CLbx"
      },
      "source": [
        "Clear previous models and generate seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3WUhdHTCL0B"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9H_y51xCQ5U"
      },
      "source": [
        "Create the generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCOO3Bn5CUV5"
      },
      "source": [
        "codings_size = 100\n",
        "\n",
        "dc_generator = Sequential([\n",
        "  Dense(7 * 7 * 128, input_shape=[codings_size]),\n",
        "  Reshape([7, 7, 128]),\n",
        "  BatchNormalization(),\n",
        "  Conv2DTranspose(\n",
        "      64, kernel_size=5, strides=2, padding='SAME',\n",
        "      activation='selu'),\n",
        "  BatchNormalization(),\n",
        "  Conv2DTranspose(\n",
        "      1, kernel_size=5, strides=2, padding='SAME',\n",
        "      activation='tanh'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_bPs4GRGrZf"
      },
      "source": [
        "We begin by feeding the generator small tensors (7 x 7 pixels) projected to 128 dimensions resulting in a 6,272 dimensional space. The goal is to increase tensor image size to match Fashion-MNIST images at 28 x 28 and decrease depth to 1 to match the channel size.\n",
        "\n",
        "The generator accepts codings of size of 100 and projects them to 6,272 dimensions (7 * 7 * 128). The generator then reshapes the projection to a 7 x 7 x 128 tensor, which is batch normalized and fed to a transposed convolutional layer with a stride of 2. The stride upsamples the tensor to 14 x 14 because it doubles the 7 x 7 dimension. The layer also reduces the tensor's depth from 128 to 64. The result is batch normalized again and fed to another transposed convolutional layer with a stride of 2. The stride upsamples it to 28 x 28 because it doubles the 14 x 14 dimension. The layer also reduces the tensor's depth from 64 to 1. The output tensor has shape (28, 28, 1), which is our goal because Fashion-MNIST images have shape 28 x 28 x 1. Since the final layer uses tanh activation, outputs are reshaped between the range -1 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htSVpgSjKhAV"
      },
      "source": [
        "Use the untrained generator to make images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTsTyLv6KhFg"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = dc_generator(noise)\n",
        "plot_multiple_images(generated_images, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHtgaDWyCXHq"
      },
      "source": [
        "## Create Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi4UzI3kgL2s"
      },
      "source": [
        "Create the DCGAN discriminator::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWQckB4UCXj7"
      },
      "source": [
        "dc_discriminator = Sequential([\n",
        "  Conv2D(64, kernel_size=5, strides=2, padding='SAME',\n",
        "         activation=LeakyReLU(0.2),\n",
        "         input_shape=[28, 28, 1]),\n",
        "  Dropout(0.4),\n",
        "  Conv2D(128, kernel_size=5, strides=2, padding='SAME',\n",
        "         activation=LeakyReLU(0.2)),\n",
        "  Dropout(0.4),\n",
        "  Flatten(),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Xs7tUkM5uD"
      },
      "source": [
        "The discriminator looks like a regular CNN for binary classification (the final Dense layer is 1), except we use strides to downsample instead of max pooling layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoVRzEcFCb_C"
      },
      "source": [
        "## Create the DCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FLhHtOUgX1k"
      },
      "source": [
        "Create the DCGAN from the DCGAN generator and discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC5-O80i_Dwx"
      },
      "source": [
        "dcgan = Sequential([dc_generator, dc_discriminator])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8miDZntWCsnE"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulAbmDKYgfFU"
      },
      "source": [
        "Compile the DCGAN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI8I8lgMANHV"
      },
      "source": [
        "dc_discriminator.compile(\n",
        "    loss='binary_crossentropy', optimizer='rmsprop')\n",
        "dc_discriminator.trainable = False\n",
        "dcgan.compile(loss='binary_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A8BDyvjC2pL"
      },
      "source": [
        "## Reshape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FkYD1VSgib7"
      },
      "source": [
        "Since outputs range from -1 to 1 due to tanh activation in the final layer of the generator, rescale the training set to the same range and rescale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsNJeR1VAPJO"
      },
      "source": [
        "images_dcgan = tf.reshape(\n",
        "    images, [-1, 28, 28, 1]) * 2. - 1.\n",
        "images_dcgan.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zRsNKJ9DTV9"
      },
      "source": [
        "## Build the Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J589HdjoglV8"
      },
      "source": [
        "Build the pipeline with batch size of 32:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyvo2C1uAVdO"
      },
      "source": [
        "batch_size = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(images_dcgan)\n",
        "dataset = dataset.shuffle(1000)\n",
        "dataset = dataset.batch(\n",
        "    batch_size, drop_remainder=True).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI4GJ41bDW5k"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BynWEsAMgoq1"
      },
      "source": [
        "Train for five epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAdFGyjbAZRu"
      },
      "source": [
        "n = 5\n",
        "\n",
        "train_gan(\n",
        "    dcgan, dataset, batch_size, codings_size, n_epochs=n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiUSMrHH5DIF"
      },
      "source": [
        "Wow! Much better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAsxRmqJJXP9"
      },
      "source": [
        "## Generate Images with the Trained Generator\n",
        "\n",
        "Use the trained DC generator to make images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX2ziIb9Ghom"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "noise = tf.random.normal(shape=[batch_size, codings_size])\n",
        "generated_images = dc_generator(noise)\n",
        "plot_multiple_images(generated_images, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Uah3FMvH5m"
      },
      "source": [
        "# Deep Convolutional GAN with Large Images\n",
        "\n",
        "A very simple Deep Convolutional GAN worked well with Fashion-MNIST. But, images in this dataset are small and grayscale. Let's see how well a Deep Convolutional GAN works with the **rock_paper_scissors** dataset, which contains large color images of hands playing the rock, paper, scissor game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS1n8NCDs4Qg"
      },
      "source": [
        "Load the dataset to inspect its metadata:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkRzlFrMi4W-"
      },
      "source": [
        "rps, info = tfds.load('rock_paper_scissors', with_info=True,\n",
        "                      split='train', try_gcs=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOzNx9Mes82H"
      },
      "source": [
        "Inspect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV9Hi2okirxD"
      },
      "source": [
        "info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui3yCEVVs_o5"
      },
      "source": [
        "We now know the size of the train set, which is all that we need.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY0wxS1XvPvL"
      },
      "source": [
        "Get the class labels and number of classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3R2R75OvP0l"
      },
      "source": [
        "num_classes = info.features['label'].num_classes\n",
        "classes = info.features['label'].names\n",
        "classes, num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDfIs3eltJuj"
      },
      "source": [
        "Visualize with **show_examples**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgN9ulM5ir0t"
      },
      "source": [
        "fig = tfds.show_examples(rps, info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdtgSeEOtV4y"
      },
      "source": [
        "## Load the Dataset for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMaxfDaPtbdK"
      },
      "source": [
        "Load the dataset as numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HN4R1guolBt"
      },
      "source": [
        "(x_train_img, _), (x_test_img, _) = tfds.as_numpy(\n",
        "    tfds.load('rock_paper_scissors', split=['train','test'],\n",
        "              batch_size=-1, as_supervised=True,\n",
        "              try_gcs=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5N_t9Xdtfqh"
      },
      "source": [
        "Inspect shapes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DNwQVVYo1b2"
      },
      "source": [
        "for element in range(10):\n",
        "  print (x_train_img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRiOrB3KtiD9"
      },
      "source": [
        "The dataset contains 2,520 300 x 300 x 3 images. Since images are of the same size, we don't have to resize. However, we resize images for another reason later in this notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPUM8lNlseC8"
      },
      "source": [
        "## Shrink the Dataset\n",
        "\n",
        "Shrink the dataset to a power of 2 to guarantee equal batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOavZNU7rkAn"
      },
      "source": [
        "x_train_rps = x_train_img[:2048]\n",
        "len(x_train_rps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBtNZ6kxt-6I"
      },
      "source": [
        "## Reformat Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwKymN9guA7P"
      },
      "source": [
        "Create a function to resize and scale images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr72yUbQo1gy"
      },
      "source": [
        "IMAGE_RES = 256\n",
        "\n",
        "def format_image(image):\n",
        "  image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/255.0\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKcS-B-ruEfX"
      },
      "source": [
        "Although we don't have to resize images, we do so to make it easier to create a model with the appropriate number of neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvM_1f3C1x5C"
      },
      "source": [
        "Although images are all 300 x 300, resize to a power of 2 for convenience. Our generator begins by projecting relatively small images onto many dimensions with a large depth size and gradually increases image size and decreases depth size until it outputs the image size we want. So, it's easier to work with values divisible by and multiplicative of 2. Deep learning models also perform better when the number of neurons in a model are based on the power of 2.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsmn87DnuYdw"
      },
      "source": [
        "## Build the Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk9kVefcua7g"
      },
      "source": [
        "Create tensors from the feature images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhEuhOwWo1jL"
      },
      "source": [
        "train_slice = tf.data.Dataset.from_tensor_slices(x_train_rps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfGalvrAuhMq"
      },
      "source": [
        "Shuffle, reformat, batch, cache, and prefetch train data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHGpETOoolFU"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_SIZE = 500\n",
        "\n",
        "train_rps = (train_slice.\n",
        "             shuffle(SHUFFLE_SIZE).\n",
        "             map(format_image).\n",
        "             batch(BATCH_SIZE).\n",
        "             cache().\n",
        "             prefetch(1))\n",
        "train_rps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyS_zRYowbzw"
      },
      "source": [
        "Visualize images from a batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYXnqOTXwb6L"
      },
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images in train_rps.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i])\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ-0U5bnuoAi"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tReckpourDp"
      },
      "source": [
        "Clear models and generate a seed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0VhbZdEolHc"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOGVP1NZuu6w"
      },
      "source": [
        "Create the generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "811HNjIMqHL0"
      },
      "source": [
        "codings_size = 100\n",
        "\n",
        "gencolor = Sequential([\n",
        "  Dense(32 * 32 * 256, input_shape=[codings_size]),\n",
        "  Reshape([32, 32, 256]),\n",
        "  BatchNormalization(),\n",
        "  Conv2DTranspose(128, kernel_size=5, strides=2, padding='SAME',\n",
        "                  activation='selu'),\n",
        "  BatchNormalization(),\n",
        "  Conv2DTranspose(64, kernel_size=5, strides=2, padding='SAME',\n",
        "                  activation='selu'),\n",
        "  BatchNormalization(),\n",
        "  Conv2DTranspose(3, kernel_size=5, strides=2, padding='SAME',\n",
        "                  activation='tanh'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD83YQjBux8Y"
      },
      "source": [
        "Notice that the number of neurons are based on the powers of 2 with one exception. The final layer contains three neurons to represent the number of classes.\n",
        "\n",
        "Begin by feeding the generator a bit bigger (32 x 32) tensors projected to 256 dimensions resulting in a 262,144 dimensional space. The generator reshapes the projection to a 32 x 32 x 256 tensor, which is batch normalized and fed to a transposed convolutional layer with a stride of 2. The stride upsamples the tensor to 64 x 64 because it doubles the 32 x 32 dimension. The layer also reduces the tensor's depth from 256 to 128. The result is batch normalized again and fed to another transposed convolutional layer with a stride of 2. The stride upsamples it to 128 x 128 because it doubles the 64 x 64 dimension. The layer also reduces the tensor's depth from 128 to 64. The result is batch normalized again and fed to another transposed convolutional layer with a stride of 2. The stride upsamples it to 256 x 256 because it doubles the 128 x 128 dimension. The layer also reduces the tensor's depth from 64 to 3. The output tensor has shape (256, 256, 3), which is our goal because we resize images to 256 x 256 x 3. Since the final layer uses tanh activation, outputs are reshaped between the range -1 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQDcoPc8w30l"
      },
      "source": [
        "Inspect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTcD7nAqqHOa"
      },
      "source": [
        "gencolor.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKLhTjMow1C2"
      },
      "source": [
        "Build the discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftjeNN2cqHSt"
      },
      "source": [
        "discolor = Sequential([\n",
        "  Conv2D(64, kernel_size=5, strides=2, padding='SAME',\n",
        "         activation=LeakyReLU(0.2),\n",
        "         input_shape=[256, 256, 3]),\n",
        "  Dropout(0.4),\n",
        "  Conv2D(128, kernel_size=5, strides=2, padding='SAME',\n",
        "         activation=LeakyReLU(0.2)),\n",
        "  Dropout(0.4),\n",
        "  Flatten(),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnTVNlVn2VKh"
      },
      "source": [
        "The discriminator accepts 256 x 256 x 3 images, downsamples tensors to 128 x 128, and increases depth from 32 to 64. Tensors are fed to a transposed convolutional layer, which downsamples tensors to 64 x 64 and increases depth to 128. The result is flattend to a 524,288 dimensional space (64 x 64 x 128). Since we are conducting binary classification, the final Dense layer includes **1** neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss5HfSNLw5r8"
      },
      "source": [
        "Inspect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPbcEt6rqHUn"
      },
      "source": [
        "discolor.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZd_x5Nw7Rl"
      },
      "source": [
        "Create the DCGAN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU1_SeLYqEAD"
      },
      "source": [
        "dcgan_color = Sequential([gencolor, discolor])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBjml7MSw9-e"
      },
      "source": [
        "## Compile:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHnVI8ElxE12"
      },
      "source": [
        "Compile with **binary_crossentropy**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYx5o6k0qEC5"
      },
      "source": [
        "discolor.compile(\n",
        "    loss='binary_crossentropy', optimizer='rmsprop')\n",
        "discolor.trainable = False\n",
        "dcgan_color.compile(\n",
        "    loss='binary_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZKXdZ2axJn7"
      },
      "source": [
        "## Rescale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5egQgcLxOAv"
      },
      "source": [
        "Create a function to rescale:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c5Z2A9tqEFa"
      },
      "source": [
        "def rescale(image):\n",
        "  image = tf.math.multiply(image, image * 2. -1)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cXqGOcVxRA_"
      },
      "source": [
        "Rescale images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffUCbQkeqmuP"
      },
      "source": [
        "train_color = train_rps.map(rescale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh03QJrJxTcp"
      },
      "source": [
        "## Model Data and Generate Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEYNvR9jxcoR"
      },
      "source": [
        "Create a Function to plot images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U44wloywqmxA"
      },
      "source": [
        "def plot_color(images, n_cols=None):\n",
        "  n_cols = n_cols or len(images)\n",
        "  n_rows = (len(images) - 1) // n_cols + 1\n",
        "  images = np.clip(images, 0, 1)\n",
        "  if images.shape[-1] == 1:\n",
        "    images = np.squeeze(images, axis=-1)\n",
        "  plt.figure(figsize=(n_cols, n_rows))\n",
        "  for index, image in enumerate(images):\n",
        "    plt.subplot(n_rows, n_cols, index + 1)\n",
        "    plt.imshow(image, cmap='binary')\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acIR-qQ_xftQ"
      },
      "source": [
        "Create a function to train the GAN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqYyRVVwqmzk"
      },
      "source": [
        "def train_gan(\n",
        "    gan, dataset, batch_size, codings_size, n_epochs=50):\n",
        "  generator, discriminator = gan.layers\n",
        "  for epoch in range(n_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
        "    for X_batch in dataset:\n",
        "      # phase 1 - training the discriminator\n",
        "      noise = tf.random.normal(\n",
        "          shape=[batch_size, codings_size])\n",
        "      generated_images = generator(noise)\n",
        "      X_fake_and_real = tf.concat(\n",
        "          [generated_images, X_batch], axis=0)\n",
        "      y1 = tf.constant(\n",
        "          [[0.]] * batch_size + [[1.]] * batch_size)\n",
        "      discriminator.trainable = True\n",
        "      discriminator.train_on_batch(X_fake_and_real, y1)\n",
        "      # phase 2 - training the generator\n",
        "      noise = tf.random.normal(\n",
        "          shape=[batch_size, codings_size])\n",
        "      y2 = tf.constant([[1.]] * batch_size)\n",
        "      discriminator.trainable = False\n",
        "      gan.train_on_batch(noise, y2)\n",
        "    plot_color(generated_images, 8)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTaGKfQoxk_H"
      },
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndGXW6fKqm2h"
      },
      "source": [
        "train_gan(\n",
        "    dcgan_color, train_color, BATCH_SIZE, codings_size, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTC6M_jO52if"
      },
      "source": [
        "Generated images are not produced satisfactorily because the generator and discriminator are much too simple. But, it's a start."
      ]
    }
  ]
}